{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:07.780592Z",
     "iopub.status.busy": "2020-10-03T15:52:07.779753Z",
     "iopub.status.idle": "2020-10-03T15:52:10.557379Z",
     "shell.execute_reply": "2020-10-03T15:52:10.556561Z"
    },
    "papermill": {
     "duration": 2.797961,
     "end_time": "2020-10-03T15:52:10.557511",
     "exception": false,
     "start_time": "2020-10-03T15:52:07.759550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "#PCA\n",
    "pca_num = 20\n",
    "#Autoencoder\n",
    "autoencoder_batch_size = 100\n",
    "autoenecoder_latents = 40 \n",
    "autoencoder_val_size = 400\n",
    "autoencoder_epochs = 60\n",
    "autoencoder_learning_rate = 0.005\n",
    "autoencoder_hidden_size_1 = 500\n",
    "autoencoder_hidden_size_2 = 200\n",
    "autoencoder_hidden_size_3 = 150\n",
    "#TabNet\n",
    "tabnet_batch_size = 100\n",
    "tabnet_val_size = 2000\n",
    "tabnet_learning_rate = 2e-2\n",
    "tabnet_weight_decay = 1e-5\n",
    "decision_layer_size = 24\n",
    "mask_attention_layer_size = 24\n",
    "#Constants\n",
    "feature_size = 874\n",
    "lable_size = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:10.983819Z",
     "iopub.status.busy": "2020-10-03T15:52:10.982835Z",
     "iopub.status.idle": "2020-10-03T15:52:17.157244Z",
     "shell.execute_reply": "2020-10-03T15:52:17.156439Z"
    },
    "papermill": {
     "duration": 6.195155,
     "end_time": "2020-10-03T15:52:17.157383",
     "exception": false,
     "start_time": "2020-10-03T15:52:10.962228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('Data/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('Data/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('Data/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('Data/test_features.csv')\n",
    "submission = pd.read_csv('Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.240033Z",
     "iopub.status.busy": "2020-10-03T15:52:17.238727Z",
     "iopub.status.idle": "2020-10-03T15:52:17.548227Z",
     "shell.execute_reply": "2020-10-03T15:52:17.548966Z"
    },
    "papermill": {
     "duration": 0.334094,
     "end_time": "2020-10-03T15:52:17.549146",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.215052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 876) (3982, 876)\n",
      "(21948, 1082) (3982, 876)\n"
     ]
    }
   ],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "# constrcut train&test except 'cp_type'=='ctl_vehicle' data\n",
    "print(train_features.shape, test_features.shape)\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.586955Z",
     "iopub.status.busy": "2020-10-03T15:52:17.586047Z",
     "iopub.status.idle": "2020-10-03T15:52:17.596968Z",
     "shell.execute_reply": "2020-10-03T15:52:17.597448Z"
    },
    "papermill": {
     "duration": 0.033725,
     "end_time": "2020-10-03T15:52:17.597588",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.563863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['cp_time'] = train['cp_time'].map({24: -1, 48: 0, 72: 1})\n",
    "train['cp_dose'] = train['cp_dose'].map({'D1': -0.5, 'D2': 0.5})\n",
    "\n",
    "test['cp_time'] = test['cp_time'].map({24: -1, 48: 0, 72: 1})\n",
    "test['cp_dose'] = test['cp_dose'].map({'D1': -0.5, 'D2': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.636939Z",
     "iopub.status.busy": "2020-10-03T15:52:17.635584Z",
     "iopub.status.idle": "2020-10-03T15:52:24.446656Z",
     "shell.execute_reply": "2020-10-03T15:52:24.447816Z"
    },
    "papermill": {
     "duration": 6.834381,
     "end_time": "2020-10-03T15:52:24.448028",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.613647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.to_numpy()\n",
    "test = test.to_numpy()\n",
    "dist_len = 99 + 771\n",
    "for d in range(dist_len):\n",
    "    train[::, 4+d]  = preprocessing.scale(train[::, 4+d])\n",
    "    test[::, 4+d]  = preprocessing.scale(test[::, 4+d])\n",
    "train = train[::, 2:].astype('float64') \n",
    "test = test[::, 2:].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = KernelPCA(n_components=pca_num, kernel='linear')\n",
    "X_transformed = transformer.fit_transform(train[::, :feature_size])\n",
    "test_transformed = transformer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: epoch : 1/60, loss = 0.6566\n",
      "Validation: epoch : 2/60, loss = 0.6138\n",
      "Validation: epoch : 3/60, loss = 0.5810\n",
      "Validation: epoch : 4/60, loss = 0.5513\n",
      "Validation: epoch : 5/60, loss = 0.5377\n",
      "Validation: epoch : 6/60, loss = 0.5346\n",
      "Validation: epoch : 7/60, loss = 0.5252\n",
      "Validation: epoch : 8/60, loss = 0.5170\n",
      "Validation: epoch : 9/60, loss = 0.5201\n",
      "Validation: epoch : 10/60, loss = 0.5284\n",
      "Validation: epoch : 11/60, loss = 0.5170\n",
      "Validation: epoch : 12/60, loss = 0.5049\n",
      "Validation: epoch : 13/60, loss = 0.5039\n",
      "Validation: epoch : 14/60, loss = 0.5002\n",
      "Validation: epoch : 15/60, loss = 0.4980\n",
      "Validation: epoch : 16/60, loss = 0.5062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-51941eac44c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader_ae\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0moptimizer_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "traningy = train[autoencoder_val_size:, :feature_size]\n",
    "valdationy = train[:autoencoder_val_size, :feature_size]\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_loader_ae = torch.utils.data.DataLoader(\n",
    "    traningy, batch_size=autoencoder_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "test_loader_ae = torch.utils.data.DataLoader(\n",
    "    valdationy, batch_size=autoencoder_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_il = nn.Linear(feature_size, autoencoder_hidden_size_1)\n",
    "        self.bnorm1 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_1)\n",
    "        self.encoder_hl1 = nn.Linear(autoencoder_hidden_size_1, autoencoder_hidden_size_2)\n",
    "        self.bnorm2 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_2)\n",
    "        self.encoder_hl2 = nn.Linear(autoencoder_hidden_size_2, autoencoder_hidden_size_3)\n",
    "        self.bnorm3 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_3)\n",
    "        self.encoder_ol = nn.Linear(autoencoder_hidden_size_3, autoenecoder_latents)\n",
    "        \n",
    "        self.bnorm4 = nn.BatchNorm1d(num_features=autoenecoder_latents)\n",
    "        self.decoder_il = nn.Linear(autoenecoder_latents, autoencoder_hidden_size_3)\n",
    "        self.bnorm5 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_3)\n",
    "        self.decoder_hl1 = nn.Linear(autoencoder_hidden_size_3, autoencoder_hidden_size_2)\n",
    "        self.bnorm6 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_2)\n",
    "        self.decoder_hl2 = nn.Linear(autoencoder_hidden_size_2, autoencoder_hidden_size_1)\n",
    "        self.bnorm7 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_1)\n",
    "        self.decoder_ol = nn.Linear(autoencoder_hidden_size_1, feature_size)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward_encoder(self, x):\n",
    "        x = self.encoder_il(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.encoder_hl1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm2(x)\n",
    "        x = self.encoder_hl2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm3(x)\n",
    "        emb = self.encoder_ol(x)\n",
    "        return emb\n",
    "    \n",
    "    def forward_decoder(self, emb):    \n",
    "        x = self.bnorm4(emb)\n",
    "        x = self.decoder_il(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm5(x)\n",
    "        x = self.decoder_hl1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm6(x)\n",
    "        x = self.decoder_hl2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm7(x)\n",
    "        x = self.decoder_ol(x)\n",
    "        return x\n",
    "    \n",
    "model_ae = AE().to(device)\n",
    "optimizer_ae = optim.Adam(model_ae.parameters(), lr=autoencoder_learning_rate)\n",
    "\n",
    "epoch_list = []\n",
    "val_list = []\n",
    "\n",
    "for epoch in range(autoencoder_epochs):\n",
    "    train_loss_en = 0\n",
    "    train_loss_de = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for x in train_loader_ae:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x = x.view((-1, feature_size))\n",
    "        emb = model_ae.forward_encoder(x.float())\n",
    "        rec = model_ae.forward_decoder(emb)\n",
    "        # compute training reconstruction loss\n",
    "        train_loss = mse(rec.double(), x)\n",
    "\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer_ae.step()\n",
    " \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss_en\n",
    "\n",
    "    if (epoch % 1) == 0:\n",
    "        val_loss_en = 0 \n",
    "        val_loss_de = 0\n",
    "        \n",
    "        for x in test_loader_ae:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            x = x.view((-1, feature_size))\n",
    "            emb = model_ae.forward_encoder(x.float())\n",
    "            rec = model_ae.forward_decoder(emb)\n",
    "            # compute training reconstruction loss\n",
    "            val_loss = mse(rec.double(), x)\n",
    "            \n",
    "        val_loss = val_loss.cpu().detach().numpy()\n",
    "        val_list.append(val_loss)\n",
    "       \n",
    "        epoch_list.append(epoch)\n",
    "        \n",
    "        print(\"Validation: epoch : {}/{}, loss = {:.4f}\".format(epoch+1, autoencoder_epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae.eval()\n",
    "enc_ae = np.empty(shape = (train.shape[0], autoenecoder_latents))\n",
    "for i in range(enc_ae.shape[0]):\n",
    "    x = torch.from_numpy(np.asarray(train[i, :feature_size])).to(device).float()\n",
    "    x = x.view(-1, feature_size)\n",
    "    x = model_ae.forward_encoder(x)\n",
    "    enc_ae[i, ::] = np.reshape(x.cpu().detach().numpy(), (autoenecoder_latents))\n",
    "    \n",
    "enc_ae_test = np.empty(shape = (test.shape[0], autoenecoder_latents))\n",
    "for i in range(enc_ae_test.shape[0]):\n",
    "    x = torch.from_numpy(np.asarray(test[i, :feature_size])).to(device).float()\n",
    "    x = x.view(-1, feature_size)\n",
    "    x = model_ae.forward_encoder(x)\n",
    "    enc_ae_test[i, ::] = np.reshape(x.cpu().detach().numpy(), (autoenecoder_latents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:24.507435Z",
     "iopub.status.busy": "2020-10-03T15:52:24.506433Z",
     "iopub.status.idle": "2020-10-03T15:52:24.676513Z",
     "shell.execute_reply": "2020-10-03T15:52:24.674951Z"
    },
    "papermill": {
     "duration": 0.20576,
     "end_time": "2020-10-03T15:52:24.676681",
     "exception": false,
     "start_time": "2020-10-03T15:52:24.470921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_no_lables = np.concatenate((train[::, :feature_size], X_transformed, enc_ae), axis = 1)\n",
    "\n",
    "val = train_no_lables[:tabnet_val_size, ::]\n",
    "train_d = train_no_lables[tabnet_val_size:, ::]\n",
    "\n",
    "lables_train = train[tabnet_val_size:, feature_size:]\n",
    "lables_val = train[:tabnet_val_size, feature_size:]\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset( torch.Tensor(train_d), torch.Tensor(lables_train) )\n",
    "validationset = torch.utils.data.TensorDataset( torch.Tensor(val), torch.Tensor(lables_val) )\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=tabnet_batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    validationset, batch_size=tabnet_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"val_loss\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "model = TabNetRegressor(n_d=decision_layer_size, n_a=mask_attention_layer_size, n_steps=1, lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=tabnet_learning_rate, weight_decay=tabnet_weight_decay), mask_type='entmax', \n",
    "                                    scheduler_params=dict(milestones=[50, 100, 150], gamma=0.9), \n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "model.fit(\n",
    "  X_train=train_d, y_train=lables_train,\n",
    "  eval_set=[(val, lables_val)],\n",
    "  loss_fn = torch.nn.BCEWithLogitsLoss(),\n",
    "  eval_metric = [LogitsLogLoss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = np.concatenate((test[::, :feature_size], test_transformed, enc_ae_test), axis = 1)\n",
    "pred_loader = torch.utils.data.DataLoader(train_aug, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:54.601321Z",
     "iopub.status.busy": "2020-10-03T15:52:54.600121Z",
     "iopub.status.idle": "2020-10-03T15:52:54.695740Z",
     "shell.execute_reply": "2020-10-03T15:52:54.695173Z"
    },
    "papermill": {
     "duration": 0.126804,
     "end_time": "2020-10-03T15:52:54.695848",
     "exception": false,
     "start_time": "2020-10-03T15:52:54.569044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_encode = np.empty(shape = (test.shape[0], lable_size))\n",
    "i = 1\n",
    "for x in pred_loader:\n",
    "    x = x.to(device)\n",
    "    outputs = model.predict(x.float())\n",
    "    pred_encode[((i-1)*(outputs.shape[0])):(i*(outputs.shape[0])), ::] = 1 / (1 + np.exp(-outputs))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:54.751705Z",
     "iopub.status.busy": "2020-10-03T15:52:54.751027Z",
     "iopub.status.idle": "2020-10-03T15:52:54.790527Z",
     "shell.execute_reply": "2020-10-03T15:52:54.789901Z"
    },
    "papermill": {
     "duration": 0.071463,
     "end_time": "2020-10-03T15:52:54.790658",
     "exception": false,
     "start_time": "2020-10-03T15:52:54.719195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a copy of all our training sig_ids for reference\n",
    "test_sig_ids = test_features['sig_id'].copy()\n",
    "\n",
    "# select all indices when 'cp_type' is 'ctl_vehicle'\n",
    "test_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')\n",
    "\n",
    "# change all cp_type == ctl_vehicle predictions to zero\n",
    "pred_encode[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n",
    "test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\n",
    "test_preds_df = pd.DataFrame(pred_encode, columns=train_targets_scored.columns[1:])\n",
    "test_submission = pd.concat([test_submission, test_preds_df], axis=1)\n",
    "test_submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:54.849482Z",
     "iopub.status.busy": "2020-10-03T15:52:54.848574Z",
     "iopub.status.idle": "2020-10-03T15:52:57.019352Z",
     "shell.execute_reply": "2020-10-03T15:52:57.018223Z"
    },
    "papermill": {
     "duration": 2.202021,
     "end_time": "2020-10-03T15:52:57.019481",
     "exception": false,
     "start_time": "2020-10-03T15:52:54.817460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 54.879322,
   "end_time": "2020-10-03T15:52:58.256922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-03T15:52:03.377600",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
