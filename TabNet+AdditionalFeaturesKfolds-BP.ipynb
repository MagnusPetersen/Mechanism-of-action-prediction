{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:07.780592Z",
     "iopub.status.busy": "2020-10-03T15:52:07.779753Z",
     "iopub.status.idle": "2020-10-03T15:52:10.557379Z",
     "shell.execute_reply": "2020-10-03T15:52:10.556561Z"
    },
    "papermill": {
     "duration": 2.797961,
     "end_time": "2020-10-03T15:52:10.557511",
     "exception": false,
     "start_time": "2020-10-03T15:52:07.759550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "#PCA\n",
    "pca_num = 20\n",
    "#Autoencoder\n",
    "autoencoder_batch_size = 100\n",
    "autoenecoder_latents = 40 \n",
    "autoencoder_val_size = 400\n",
    "autoencoder_epochs = 60\n",
    "autoencoder_learning_rate = 0.005\n",
    "autoencoder_hidden_size_1 = 500\n",
    "autoencoder_hidden_size_2 = 200\n",
    "autoencoder_hidden_size_3 = 150\n",
    "#TabNet\n",
    "n_folds = 10\n",
    "tabnet_batch_size = 100\n",
    "tabnet_learning_rate = 2e-2\n",
    "tabnet_weight_decay = 1e-5\n",
    "decision_layer_size = 32\n",
    "mask_attention_layer_size = 32\n",
    "#Blender\n",
    "blend_val_size =2000\n",
    "blender_batch_size = 100\n",
    "#Constants\n",
    "feature_size = 874\n",
    "lable_size = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:10.983819Z",
     "iopub.status.busy": "2020-10-03T15:52:10.982835Z",
     "iopub.status.idle": "2020-10-03T15:52:17.157244Z",
     "shell.execute_reply": "2020-10-03T15:52:17.156439Z"
    },
    "papermill": {
     "duration": 6.195155,
     "end_time": "2020-10-03T15:52:17.157383",
     "exception": false,
     "start_time": "2020-10-03T15:52:10.962228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('Data/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('Data/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('Data/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('Data/test_features.csv')\n",
    "submission = pd.read_csv('Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.240033Z",
     "iopub.status.busy": "2020-10-03T15:52:17.238727Z",
     "iopub.status.idle": "2020-10-03T15:52:17.548227Z",
     "shell.execute_reply": "2020-10-03T15:52:17.548966Z"
    },
    "papermill": {
     "duration": 0.334094,
     "end_time": "2020-10-03T15:52:17.549146",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.215052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 876) (3982, 876)\n",
      "(21948, 1082) (3982, 876)\n"
     ]
    }
   ],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "print(train_features.shape, test_features.shape)\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.586955Z",
     "iopub.status.busy": "2020-10-03T15:52:17.586047Z",
     "iopub.status.idle": "2020-10-03T15:52:17.596968Z",
     "shell.execute_reply": "2020-10-03T15:52:17.597448Z"
    },
    "papermill": {
     "duration": 0.033725,
     "end_time": "2020-10-03T15:52:17.597588",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.563863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['cp_time'] = train['cp_time'].map({24: -1, 48: 0, 72: 1})\n",
    "train['cp_dose'] = train['cp_dose'].map({'D1': -0.5, 'D2': 0.5})\n",
    "\n",
    "test['cp_time'] = test['cp_time'].map({24: -1, 48: 0, 72: 1})\n",
    "test['cp_dose'] = test['cp_dose'].map({'D1': -0.5, 'D2': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:17.636939Z",
     "iopub.status.busy": "2020-10-03T15:52:17.635584Z",
     "iopub.status.idle": "2020-10-03T15:52:24.446656Z",
     "shell.execute_reply": "2020-10-03T15:52:24.447816Z"
    },
    "papermill": {
     "duration": 6.834381,
     "end_time": "2020-10-03T15:52:24.448028",
     "exception": false,
     "start_time": "2020-10-03T15:52:17.613647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.to_numpy()\n",
    "test = test.to_numpy()\n",
    "dist_len = 99 + 771\n",
    "for d in range(dist_len):\n",
    "    train[::, 4+d]  = preprocessing.scale(train[::, 4+d])\n",
    "    test[::, 4+d]  = preprocessing.scale(test[::, 4+d])\n",
    "train = train[::, 2:].astype('float64') \n",
    "test = test[::, 2:].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = KernelPCA(n_components=pca_num, kernel='linear')\n",
    "X_transformed = transformer.fit_transform(train[::, :feature_size])\n",
    "test_transformed = transformer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: epoch : 1/60, loss = 0.6656\n",
      "Validation: epoch : 2/60, loss = 0.5993\n",
      "Validation: epoch : 3/60, loss = 0.5743\n",
      "Validation: epoch : 4/60, loss = 0.5554\n",
      "Validation: epoch : 5/60, loss = 0.5439\n",
      "Validation: epoch : 6/60, loss = 0.5301\n",
      "Validation: epoch : 7/60, loss = 0.5188\n",
      "Validation: epoch : 8/60, loss = 0.5129\n",
      "Validation: epoch : 9/60, loss = 0.5229\n",
      "Validation: epoch : 10/60, loss = 0.5265\n",
      "Validation: epoch : 11/60, loss = 0.5222\n",
      "Validation: epoch : 12/60, loss = 0.5064\n",
      "Validation: epoch : 13/60, loss = 0.4976\n",
      "Validation: epoch : 14/60, loss = 0.4996\n",
      "Validation: epoch : 15/60, loss = 0.4920\n",
      "Validation: epoch : 16/60, loss = 0.4948\n",
      "Validation: epoch : 17/60, loss = 0.4931\n",
      "Validation: epoch : 18/60, loss = 0.4932\n",
      "Validation: epoch : 19/60, loss = 0.4932\n",
      "Validation: epoch : 20/60, loss = 0.4856\n",
      "Validation: epoch : 21/60, loss = 0.4911\n",
      "Validation: epoch : 22/60, loss = 0.4908\n",
      "Validation: epoch : 23/60, loss = 0.4995\n",
      "Validation: epoch : 24/60, loss = 0.4948\n",
      "Validation: epoch : 25/60, loss = 0.4860\n",
      "Validation: epoch : 26/60, loss = 0.4956\n",
      "Validation: epoch : 27/60, loss = 0.4933\n",
      "Validation: epoch : 28/60, loss = 0.5067\n",
      "Validation: epoch : 29/60, loss = 0.4877\n",
      "Validation: epoch : 30/60, loss = 0.4867\n",
      "Validation: epoch : 31/60, loss = 0.4880\n",
      "Validation: epoch : 32/60, loss = 0.4873\n",
      "Validation: epoch : 33/60, loss = 0.4819\n",
      "Validation: epoch : 34/60, loss = 0.4790\n",
      "Validation: epoch : 35/60, loss = 0.4783\n",
      "Validation: epoch : 36/60, loss = 0.4825\n",
      "Validation: epoch : 37/60, loss = 0.4814\n",
      "Validation: epoch : 38/60, loss = 0.4838\n",
      "Validation: epoch : 39/60, loss = 0.4844\n",
      "Validation: epoch : 40/60, loss = 0.4845\n",
      "Validation: epoch : 41/60, loss = 0.4792\n",
      "Validation: epoch : 42/60, loss = 0.4843\n",
      "Validation: epoch : 43/60, loss = 0.4913\n",
      "Validation: epoch : 44/60, loss = 0.4855\n",
      "Validation: epoch : 45/60, loss = 0.4874\n",
      "Validation: epoch : 46/60, loss = 0.5001\n",
      "Validation: epoch : 47/60, loss = 0.4889\n",
      "Validation: epoch : 48/60, loss = 0.4823\n",
      "Validation: epoch : 49/60, loss = 0.4827\n",
      "Validation: epoch : 50/60, loss = 0.4788\n",
      "Validation: epoch : 51/60, loss = 0.4850\n",
      "Validation: epoch : 52/60, loss = 0.4789\n",
      "Validation: epoch : 53/60, loss = 0.4806\n",
      "Validation: epoch : 54/60, loss = 0.4850\n",
      "Validation: epoch : 55/60, loss = 0.4880\n",
      "Validation: epoch : 56/60, loss = 0.4922\n",
      "Validation: epoch : 57/60, loss = 0.4878\n",
      "Validation: epoch : 58/60, loss = 0.4873\n",
      "Validation: epoch : 59/60, loss = 0.4877\n",
      "Validation: epoch : 60/60, loss = 0.4862\n"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "traningy = train[autoencoder_val_size:, :feature_size]\n",
    "valdationy = train[:autoencoder_val_size, :feature_size]\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_loader_ae = torch.utils.data.DataLoader(\n",
    "    traningy, batch_size=autoencoder_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "test_loader_ae = torch.utils.data.DataLoader(\n",
    "    valdationy, batch_size=autoencoder_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_il = nn.Linear(feature_size, autoencoder_hidden_size_1)\n",
    "        self.bnorm1 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_1)\n",
    "        self.encoder_hl1 = nn.Linear(autoencoder_hidden_size_1, autoencoder_hidden_size_2)\n",
    "        self.bnorm2 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_2)\n",
    "        self.encoder_hl2 = nn.Linear(autoencoder_hidden_size_2, autoencoder_hidden_size_3)\n",
    "        self.bnorm3 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_3)\n",
    "        self.encoder_ol = nn.Linear(autoencoder_hidden_size_3, autoenecoder_latents)\n",
    "        \n",
    "        self.bnorm4 = nn.BatchNorm1d(num_features=autoenecoder_latents)\n",
    "        self.decoder_il = nn.Linear(autoenecoder_latents, autoencoder_hidden_size_3)\n",
    "        self.bnorm5 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_3)\n",
    "        self.decoder_hl1 = nn.Linear(autoencoder_hidden_size_3, autoencoder_hidden_size_2)\n",
    "        self.bnorm6 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_2)\n",
    "        self.decoder_hl2 = nn.Linear(autoencoder_hidden_size_2, autoencoder_hidden_size_1)\n",
    "        self.bnorm7 = nn.BatchNorm1d(num_features=autoencoder_hidden_size_1)\n",
    "        self.decoder_ol = nn.Linear(autoencoder_hidden_size_1, feature_size)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward_encoder(self, x):\n",
    "        x = self.encoder_il(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.encoder_hl1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm2(x)\n",
    "        x = self.encoder_hl2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm3(x)\n",
    "        emb = self.encoder_ol(x)\n",
    "        return emb\n",
    "    \n",
    "    def forward_decoder(self, emb):    \n",
    "        x = self.bnorm4(emb)\n",
    "        x = self.decoder_il(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm5(x)\n",
    "        x = self.decoder_hl1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm6(x)\n",
    "        x = self.decoder_hl2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.bnorm7(x)\n",
    "        x = self.decoder_ol(x)\n",
    "        return x\n",
    "    \n",
    "model_ae = AE().to(device)\n",
    "optimizer_ae = optim.Adam(model_ae.parameters(), lr=autoencoder_learning_rate)\n",
    "\n",
    "epoch_list = []\n",
    "val_list = []\n",
    "\n",
    "for epoch in range(autoencoder_epochs):\n",
    "    train_loss_en = 0\n",
    "    train_loss_de = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for x in train_loader_ae:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x = x.view((-1, feature_size))\n",
    "        emb = model_ae.forward_encoder(x.float())\n",
    "        rec = model_ae.forward_decoder(emb)\n",
    "        # compute training reconstruction loss\n",
    "        train_loss = mse(rec.double(), x)\n",
    "\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer_ae.step()\n",
    " \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss_en\n",
    "\n",
    "    if (epoch % 1) == 0:\n",
    "        val_loss_en = 0 \n",
    "        val_loss_de = 0\n",
    "        \n",
    "        for x in test_loader_ae:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            x = x.view((-1, feature_size))\n",
    "            emb = model_ae.forward_encoder(x.float())\n",
    "            rec = model_ae.forward_decoder(emb)\n",
    "            # compute training reconstruction loss\n",
    "            val_loss = mse(rec.double(), x)\n",
    "            \n",
    "        val_loss = val_loss.cpu().detach().numpy()\n",
    "        val_list.append(val_loss)\n",
    "       \n",
    "        epoch_list.append(epoch)\n",
    "        \n",
    "        print(\"Validation: epoch : {}/{}, loss = {:.4f}\".format(epoch+1, autoencoder_epochs, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder_il): Linear(in_features=874, out_features=500, bias=True)\n",
       "  (bnorm1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encoder_hl1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (bnorm2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encoder_hl2): Linear(in_features=200, out_features=150, bias=True)\n",
       "  (bnorm3): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encoder_ol): Linear(in_features=150, out_features=40, bias=True)\n",
       "  (bnorm4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decoder_il): Linear(in_features=40, out_features=150, bias=True)\n",
       "  (bnorm5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decoder_hl1): Linear(in_features=150, out_features=200, bias=True)\n",
       "  (bnorm6): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decoder_hl2): Linear(in_features=200, out_features=500, bias=True)\n",
       "  (bnorm7): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decoder_ol): Linear(in_features=500, out_features=874, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch = np.argmin(val_list)\n",
    "model_ae.load_state_dict(torch.load(\"./saves/ae\"+str(best_epoch)))\n",
    "model_ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae.eval()\n",
    "enc_ae = np.empty(shape = (train.shape[0], autoenecoder_latents))\n",
    "for i in range(enc_ae.shape[0]):\n",
    "    x = torch.from_numpy(np.asarray(train[i, :feature_size])).to(device).float()\n",
    "    x = x.view(-1, feature_size)\n",
    "    x = model_ae.forward_encoder(x)\n",
    "    enc_ae[i, ::] = np.reshape(x.cpu().detach().numpy(), (autoenecoder_latents))\n",
    "    \n",
    "enc_ae_test = np.empty(shape = (test.shape[0], autoenecoder_latents))\n",
    "for i in range(enc_ae_test.shape[0]):\n",
    "    x = torch.from_numpy(np.asarray(test[i, :feature_size])).to(device).float()\n",
    "    x = x.view(-1, feature_size)\n",
    "    x = model_ae.forward_encoder(x)\n",
    "    enc_ae_test[i, ::] = np.reshape(x.cpu().detach().numpy(), (autoenecoder_latents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:24.507435Z",
     "iopub.status.busy": "2020-10-03T15:52:24.506433Z",
     "iopub.status.idle": "2020-10-03T15:52:24.676513Z",
     "shell.execute_reply": "2020-10-03T15:52:24.674951Z"
    },
    "papermill": {
     "duration": 0.20576,
     "end_time": "2020-10-03T15:52:24.676681",
     "exception": false,
     "start_time": "2020-10-03T15:52:24.470921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_d = np.concatenate((train[::, :feature_size], X_transformed, enc_ae), axis = 1)\n",
    "train_aug = np.concatenate((test[::, :feature_size], test_transformed, enc_ae_test), axis = 1)\n",
    "\n",
    "lables_train = train[::, feature_size:]\n",
    "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_d), torch.Tensor(lables_train) )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=tabnet_batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "pred_loader = torch.utils.data.DataLoader(train_aug, batch_size=tabnet_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"val_loss\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:  1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36222 | val_0_val_loss: 0.03795 |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.0286  | val_0_val_loss: 0.02675 |  0:00:02s\n",
      "epoch 2  | loss: 0.02271 | val_0_val_loss: 0.02206 |  0:00:04s\n",
      "epoch 3  | loss: 0.02129 | val_0_val_loss: 0.02104 |  0:00:05s\n",
      "epoch 4  | loss: 0.02077 | val_0_val_loss: 0.02078 |  0:00:07s\n",
      "epoch 5  | loss: 0.02051 | val_0_val_loss: 0.02084 |  0:00:08s\n",
      "epoch 6  | loss: 0.02023 | val_0_val_loss: 0.02084 |  0:00:10s\n",
      "epoch 7  | loss: 0.0199  | val_0_val_loss: 0.01997 |  0:00:11s\n",
      "epoch 8  | loss: 0.01966 | val_0_val_loss: 0.02086 |  0:00:13s\n",
      "epoch 9  | loss: 0.01935 | val_0_val_loss: 0.0195  |  0:00:14s\n",
      "epoch 10 | loss: 0.01902 | val_0_val_loss: 0.01934 |  0:00:16s\n",
      "epoch 11 | loss: 0.01881 | val_0_val_loss: 0.01982 |  0:00:17s\n",
      "epoch 12 | loss: 0.01857 | val_0_val_loss: 0.0188  |  0:00:18s\n",
      "epoch 13 | loss: 0.01834 | val_0_val_loss: 0.02042 |  0:00:20s\n",
      "epoch 14 | loss: 0.0183  | val_0_val_loss: 0.01844 |  0:00:21s\n",
      "epoch 15 | loss: 0.01798 | val_0_val_loss: 0.01833 |  0:00:22s\n",
      "epoch 16 | loss: 0.01775 | val_0_val_loss: 0.01788 |  0:00:24s\n",
      "epoch 17 | loss: 0.01758 | val_0_val_loss: 0.01814 |  0:00:25s\n",
      "epoch 18 | loss: 0.01744 | val_0_val_loss: 0.01762 |  0:00:27s\n",
      "epoch 19 | loss: 0.01732 | val_0_val_loss: 0.01794 |  0:00:28s\n",
      "epoch 20 | loss: 0.01721 | val_0_val_loss: 0.01758 |  0:00:29s\n",
      "epoch 21 | loss: 0.01715 | val_0_val_loss: 0.01863 |  0:00:31s\n",
      "epoch 22 | loss: 0.01704 | val_0_val_loss: 0.02038 |  0:00:32s\n",
      "epoch 23 | loss: 0.017   | val_0_val_loss: 0.01741 |  0:00:34s\n",
      "epoch 24 | loss: 0.0169  | val_0_val_loss: 0.01944 |  0:00:35s\n",
      "epoch 25 | loss: 0.0168  | val_0_val_loss: 0.01743 |  0:00:37s\n",
      "epoch 26 | loss: 0.01676 | val_0_val_loss: 0.01775 |  0:00:38s\n",
      "epoch 27 | loss: 0.01671 | val_0_val_loss: 0.01751 |  0:00:40s\n",
      "epoch 28 | loss: 0.01662 | val_0_val_loss: 0.01755 |  0:00:41s\n",
      "epoch 29 | loss: 0.01663 | val_0_val_loss: 0.01877 |  0:00:43s\n",
      "epoch 30 | loss: 0.0165  | val_0_val_loss: 0.01738 |  0:00:44s\n",
      "epoch 31 | loss: 0.01648 | val_0_val_loss: 0.01745 |  0:00:46s\n",
      "epoch 32 | loss: 0.01657 | val_0_val_loss: 0.01885 |  0:00:47s\n",
      "epoch 33 | loss: 0.01646 | val_0_val_loss: 0.01756 |  0:00:49s\n",
      "epoch 34 | loss: 0.01645 | val_0_val_loss: 0.01715 |  0:00:50s\n",
      "epoch 35 | loss: 0.01638 | val_0_val_loss: 0.01752 |  0:00:52s\n",
      "epoch 36 | loss: 0.01633 | val_0_val_loss: 0.01711 |  0:00:53s\n",
      "epoch 37 | loss: 0.01638 | val_0_val_loss: 0.01884 |  0:00:55s\n",
      "epoch 38 | loss: 0.01632 | val_0_val_loss: 0.01918 |  0:00:56s\n",
      "epoch 39 | loss: 0.01636 | val_0_val_loss: 0.01721 |  0:00:58s\n",
      "epoch 40 | loss: 0.01615 | val_0_val_loss: 0.0171  |  0:00:59s\n",
      "epoch 41 | loss: 0.01617 | val_0_val_loss: 0.01759 |  0:01:01s\n",
      "epoch 42 | loss: 0.01623 | val_0_val_loss: 0.01753 |  0:01:02s\n",
      "epoch 43 | loss: 0.01619 | val_0_val_loss: 0.01718 |  0:01:04s\n",
      "epoch 44 | loss: 0.01614 | val_0_val_loss: 0.01721 |  0:01:05s\n",
      "epoch 45 | loss: 0.0162  | val_0_val_loss: 0.01709 |  0:01:07s\n",
      "epoch 46 | loss: 0.0161  | val_0_val_loss: 0.01739 |  0:01:08s\n",
      "epoch 47 | loss: 0.01619 | val_0_val_loss: 0.01692 |  0:01:10s\n",
      "epoch 48 | loss: 0.01631 | val_0_val_loss: 0.01737 |  0:01:12s\n",
      "epoch 49 | loss: 0.01627 | val_0_val_loss: 0.01774 |  0:01:13s\n",
      "epoch 50 | loss: 0.01597 | val_0_val_loss: 0.01701 |  0:01:14s\n",
      "epoch 51 | loss: 0.01587 | val_0_val_loss: 0.0169  |  0:01:16s\n",
      "epoch 52 | loss: 0.01607 | val_0_val_loss: 0.01736 |  0:01:17s\n",
      "epoch 53 | loss: 0.01602 | val_0_val_loss: 0.0172  |  0:01:19s\n",
      "epoch 54 | loss: 0.01582 | val_0_val_loss: 0.01672 |  0:01:20s\n",
      "epoch 55 | loss: 0.01604 | val_0_val_loss: 0.01723 |  0:01:22s\n",
      "epoch 56 | loss: 0.01605 | val_0_val_loss: 0.0184  |  0:01:24s\n",
      "epoch 57 | loss: 0.01597 | val_0_val_loss: 0.01704 |  0:01:25s\n",
      "epoch 58 | loss: 0.01588 | val_0_val_loss: 0.01691 |  0:01:27s\n",
      "epoch 59 | loss: 0.01595 | val_0_val_loss: 0.01699 |  0:01:28s\n",
      "epoch 60 | loss: 0.01589 | val_0_val_loss: 0.01709 |  0:01:30s\n",
      "epoch 61 | loss: 0.01576 | val_0_val_loss: 0.01721 |  0:01:32s\n",
      "epoch 62 | loss: 0.01588 | val_0_val_loss: 0.01683 |  0:01:33s\n",
      "epoch 63 | loss: 0.01582 | val_0_val_loss: 0.017   |  0:01:35s\n",
      "epoch 64 | loss: 0.01587 | val_0_val_loss: 0.01697 |  0:01:37s\n",
      "\n",
      "Early stopping occured at epoch 64 with best_epoch = 54 and best_val_0_val_loss = 0.01672\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36078 | val_0_val_loss: 0.03795 |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.02888 | val_0_val_loss: 0.02691 |  0:00:03s\n",
      "epoch 2  | loss: 0.02306 | val_0_val_loss: 0.0215  |  0:00:05s\n",
      "epoch 3  | loss: 0.02132 | val_0_val_loss: 0.02076 |  0:00:06s\n",
      "epoch 4  | loss: 0.02085 | val_0_val_loss: 0.02051 |  0:00:08s\n",
      "epoch 5  | loss: 0.02062 | val_0_val_loss: 0.02039 |  0:00:10s\n",
      "epoch 6  | loss: 0.02043 | val_0_val_loss: 0.02018 |  0:00:11s\n",
      "epoch 7  | loss: 0.02017 | val_0_val_loss: 0.01999 |  0:00:13s\n",
      "epoch 8  | loss: 0.01984 | val_0_val_loss: 0.01973 |  0:00:14s\n",
      "epoch 9  | loss: 0.01962 | val_0_val_loss: 0.01953 |  0:00:15s\n",
      "epoch 10 | loss: 0.01932 | val_0_val_loss: 0.02042 |  0:00:17s\n",
      "epoch 11 | loss: 0.01902 | val_0_val_loss: 0.01922 |  0:00:18s\n",
      "epoch 12 | loss: 0.01866 | val_0_val_loss: 0.01916 |  0:00:20s\n",
      "epoch 13 | loss: 0.0185  | val_0_val_loss: 0.01898 |  0:00:22s\n",
      "epoch 14 | loss: 0.01819 | val_0_val_loss: 0.01857 |  0:00:23s\n",
      "epoch 15 | loss: 0.01805 | val_0_val_loss: 0.01899 |  0:00:25s\n",
      "epoch 16 | loss: 0.01787 | val_0_val_loss: 0.01839 |  0:00:26s\n",
      "epoch 17 | loss: 0.01783 | val_0_val_loss: 0.01809 |  0:00:28s\n",
      "epoch 18 | loss: 0.01782 | val_0_val_loss: 0.01824 |  0:00:29s\n",
      "epoch 19 | loss: 0.01759 | val_0_val_loss: 0.01888 |  0:00:31s\n",
      "epoch 20 | loss: 0.01764 | val_0_val_loss: 0.02129 |  0:00:32s\n",
      "epoch 21 | loss: 0.0176  | val_0_val_loss: 0.0181  |  0:00:34s\n",
      "epoch 22 | loss: 0.01738 | val_0_val_loss: 0.01779 |  0:00:35s\n",
      "epoch 23 | loss: 0.01738 | val_0_val_loss: 0.01784 |  0:00:37s\n",
      "epoch 24 | loss: 0.01724 | val_0_val_loss: 0.01937 |  0:00:38s\n",
      "epoch 25 | loss: 0.01726 | val_0_val_loss: 0.01948 |  0:00:40s\n",
      "epoch 26 | loss: 0.01718 | val_0_val_loss: 0.0176  |  0:00:41s\n",
      "epoch 27 | loss: 0.01701 | val_0_val_loss: 0.01818 |  0:00:43s\n",
      "epoch 28 | loss: 0.01702 | val_0_val_loss: 0.01916 |  0:00:44s\n",
      "epoch 29 | loss: 0.01685 | val_0_val_loss: 0.01748 |  0:00:46s\n",
      "epoch 30 | loss: 0.01678 | val_0_val_loss: 0.01868 |  0:00:48s\n",
      "epoch 31 | loss: 0.01686 | val_0_val_loss: 0.01891 |  0:00:49s\n",
      "epoch 32 | loss: 0.01668 | val_0_val_loss: 0.01964 |  0:00:51s\n",
      "epoch 33 | loss: 0.01672 | val_0_val_loss: 0.0175  |  0:00:52s\n",
      "epoch 34 | loss: 0.01659 | val_0_val_loss: 0.01881 |  0:00:54s\n",
      "epoch 35 | loss: 0.0166  | val_0_val_loss: 0.01788 |  0:00:55s\n",
      "epoch 36 | loss: 0.01643 | val_0_val_loss: 0.0186  |  0:00:56s\n",
      "epoch 37 | loss: 0.01642 | val_0_val_loss: 0.01867 |  0:00:58s\n",
      "epoch 38 | loss: 0.01639 | val_0_val_loss: 0.01731 |  0:00:59s\n",
      "epoch 39 | loss: 0.01645 | val_0_val_loss: 0.01782 |  0:01:01s\n",
      "epoch 40 | loss: 0.01647 | val_0_val_loss: 0.01727 |  0:01:03s\n",
      "epoch 41 | loss: 0.01621 | val_0_val_loss: 0.01795 |  0:01:04s\n",
      "epoch 42 | loss: 0.01622 | val_0_val_loss: 0.01879 |  0:01:06s\n",
      "epoch 43 | loss: 0.01618 | val_0_val_loss: 0.01751 |  0:01:07s\n",
      "epoch 44 | loss: 0.01626 | val_0_val_loss: 0.01843 |  0:01:09s\n",
      "epoch 45 | loss: 0.01619 | val_0_val_loss: 0.02001 |  0:01:10s\n",
      "epoch 46 | loss: 0.01613 | val_0_val_loss: 0.0172  |  0:01:12s\n",
      "epoch 47 | loss: 0.01617 | val_0_val_loss: 0.01843 |  0:01:13s\n",
      "epoch 48 | loss: 0.01619 | val_0_val_loss: 0.01831 |  0:01:15s\n",
      "epoch 49 | loss: 0.01625 | val_0_val_loss: 0.01871 |  0:01:16s\n",
      "epoch 50 | loss: 0.01613 | val_0_val_loss: 0.01764 |  0:01:18s\n",
      "epoch 51 | loss: 0.01605 | val_0_val_loss: 0.01712 |  0:01:19s\n",
      "epoch 52 | loss: 0.016   | val_0_val_loss: 0.01833 |  0:01:21s\n",
      "epoch 53 | loss: 0.01616 | val_0_val_loss: 0.01799 |  0:01:22s\n",
      "epoch 54 | loss: 0.01603 | val_0_val_loss: 0.01702 |  0:01:24s\n",
      "epoch 55 | loss: 0.01594 | val_0_val_loss: 0.01905 |  0:01:26s\n",
      "epoch 56 | loss: 0.01599 | val_0_val_loss: 0.01738 |  0:01:27s\n",
      "epoch 57 | loss: 0.01605 | val_0_val_loss: 0.01705 |  0:01:28s\n",
      "epoch 58 | loss: 0.01589 | val_0_val_loss: 0.01704 |  0:01:30s\n",
      "epoch 59 | loss: 0.01588 | val_0_val_loss: 0.01684 |  0:01:32s\n",
      "epoch 60 | loss: 0.01585 | val_0_val_loss: 0.01777 |  0:01:34s\n",
      "epoch 61 | loss: 0.01602 | val_0_val_loss: 0.01698 |  0:01:35s\n",
      "epoch 62 | loss: 0.01584 | val_0_val_loss: 0.01691 |  0:01:37s\n",
      "epoch 63 | loss: 0.01587 | val_0_val_loss: 0.01815 |  0:01:39s\n",
      "epoch 64 | loss: 0.0161  | val_0_val_loss: 0.01696 |  0:01:40s\n",
      "epoch 65 | loss: 0.01603 | val_0_val_loss: 0.01735 |  0:01:42s\n",
      "epoch 66 | loss: 0.01594 | val_0_val_loss: 0.01684 |  0:01:44s\n",
      "epoch 67 | loss: 0.01573 | val_0_val_loss: 0.01697 |  0:01:45s\n",
      "epoch 68 | loss: 0.01578 | val_0_val_loss: 0.0179  |  0:01:47s\n",
      "epoch 69 | loss: 0.01589 | val_0_val_loss: 0.01737 |  0:01:48s\n",
      "\n",
      "Early stopping occured at epoch 69 with best_epoch = 59 and best_val_0_val_loss = 0.01684\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36474 | val_0_val_loss: 0.04258 |  0:00:01s\n",
      "epoch 1  | loss: 0.02895 | val_0_val_loss: 0.02695 |  0:00:02s\n",
      "epoch 2  | loss: 0.02295 | val_0_val_loss: 0.02198 |  0:00:04s\n",
      "epoch 3  | loss: 0.02138 | val_0_val_loss: 0.02132 |  0:00:05s\n",
      "epoch 4  | loss: 0.0209  | val_0_val_loss: 0.0209  |  0:00:07s\n",
      "epoch 5  | loss: 0.02058 | val_0_val_loss: 0.02061 |  0:00:09s\n",
      "epoch 6  | loss: 0.02029 | val_0_val_loss: 0.02072 |  0:00:10s\n",
      "epoch 7  | loss: 0.02002 | val_0_val_loss: 0.01999 |  0:00:12s\n",
      "epoch 8  | loss: 0.01964 | val_0_val_loss: 0.01986 |  0:00:13s\n",
      "epoch 9  | loss: 0.01927 | val_0_val_loss: 0.0194  |  0:00:15s\n",
      "epoch 10 | loss: 0.01884 | val_0_val_loss: 0.02017 |  0:00:16s\n",
      "epoch 11 | loss: 0.01851 | val_0_val_loss: 0.02046 |  0:00:18s\n",
      "epoch 12 | loss: 0.01825 | val_0_val_loss: 0.01903 |  0:00:19s\n",
      "epoch 13 | loss: 0.01806 | val_0_val_loss: 0.01868 |  0:00:21s\n",
      "epoch 14 | loss: 0.01786 | val_0_val_loss: 0.01889 |  0:00:22s\n",
      "epoch 15 | loss: 0.01786 | val_0_val_loss: 0.01867 |  0:00:24s\n",
      "epoch 16 | loss: 0.0176  | val_0_val_loss: 0.01844 |  0:00:25s\n",
      "epoch 17 | loss: 0.01758 | val_0_val_loss: 0.01818 |  0:00:27s\n",
      "epoch 18 | loss: 0.01748 | val_0_val_loss: 0.01924 |  0:00:28s\n",
      "epoch 19 | loss: 0.01728 | val_0_val_loss: 0.01794 |  0:00:30s\n",
      "epoch 20 | loss: 0.01727 | val_0_val_loss: 0.01811 |  0:00:31s\n",
      "epoch 21 | loss: 0.01719 | val_0_val_loss: 0.01932 |  0:00:33s\n",
      "epoch 22 | loss: 0.01708 | val_0_val_loss: 0.01791 |  0:00:34s\n",
      "epoch 23 | loss: 0.01704 | val_0_val_loss: 0.01876 |  0:00:36s\n",
      "epoch 24 | loss: 0.01693 | val_0_val_loss: 0.01784 |  0:00:37s\n",
      "epoch 25 | loss: 0.01697 | val_0_val_loss: 0.01777 |  0:00:39s\n",
      "epoch 26 | loss: 0.01695 | val_0_val_loss: 0.01838 |  0:00:40s\n",
      "epoch 27 | loss: 0.01678 | val_0_val_loss: 0.01804 |  0:00:42s\n",
      "epoch 28 | loss: 0.01674 | val_0_val_loss: 0.0193  |  0:00:43s\n",
      "epoch 29 | loss: 0.01667 | val_0_val_loss: 0.01765 |  0:00:45s\n",
      "epoch 30 | loss: 0.01668 | val_0_val_loss: 0.01759 |  0:00:46s\n",
      "epoch 31 | loss: 0.01665 | val_0_val_loss: 0.01999 |  0:00:48s\n",
      "epoch 32 | loss: 0.0165  | val_0_val_loss: 0.01802 |  0:00:49s\n",
      "epoch 33 | loss: 0.01653 | val_0_val_loss: 0.01786 |  0:00:51s\n",
      "epoch 34 | loss: 0.01653 | val_0_val_loss: 0.0192  |  0:00:53s\n",
      "epoch 35 | loss: 0.01648 | val_0_val_loss: 0.01747 |  0:00:54s\n",
      "epoch 36 | loss: 0.01635 | val_0_val_loss: 0.01913 |  0:00:56s\n",
      "epoch 37 | loss: 0.01629 | val_0_val_loss: 0.01746 |  0:00:57s\n",
      "epoch 38 | loss: 0.01624 | val_0_val_loss: 0.01745 |  0:00:59s\n",
      "epoch 39 | loss: 0.01637 | val_0_val_loss: 0.01895 |  0:01:00s\n",
      "epoch 40 | loss: 0.01632 | val_0_val_loss: 0.01741 |  0:01:02s\n",
      "epoch 41 | loss: 0.01633 | val_0_val_loss: 0.01901 |  0:01:04s\n",
      "epoch 42 | loss: 0.01622 | val_0_val_loss: 0.01739 |  0:01:05s\n",
      "epoch 43 | loss: 0.01622 | val_0_val_loss: 0.01741 |  0:01:07s\n",
      "epoch 44 | loss: 0.01621 | val_0_val_loss: 0.01854 |  0:01:08s\n",
      "epoch 45 | loss: 0.01618 | val_0_val_loss: 0.01748 |  0:01:10s\n",
      "epoch 46 | loss: 0.01613 | val_0_val_loss: 0.01733 |  0:01:11s\n",
      "epoch 47 | loss: 0.0162  | val_0_val_loss: 0.01742 |  0:01:13s\n",
      "epoch 48 | loss: 0.01607 | val_0_val_loss: 0.01765 |  0:01:14s\n",
      "epoch 49 | loss: 0.01602 | val_0_val_loss: 0.01739 |  0:01:16s\n",
      "epoch 50 | loss: 0.01604 | val_0_val_loss: 0.01734 |  0:01:17s\n",
      "epoch 51 | loss: 0.01597 | val_0_val_loss: 0.01745 |  0:01:19s\n",
      "epoch 52 | loss: 0.01588 | val_0_val_loss: 0.01743 |  0:01:21s\n",
      "epoch 53 | loss: 0.01588 | val_0_val_loss: 0.01761 |  0:01:22s\n",
      "epoch 54 | loss: 0.01589 | val_0_val_loss: 0.01776 |  0:01:23s\n",
      "epoch 55 | loss: 0.01593 | val_0_val_loss: 0.01818 |  0:01:25s\n",
      "epoch 56 | loss: 0.016   | val_0_val_loss: 0.01717 |  0:01:26s\n",
      "epoch 57 | loss: 0.01591 | val_0_val_loss: 0.01729 |  0:01:28s\n",
      "epoch 58 | loss: 0.01585 | val_0_val_loss: 0.01719 |  0:01:29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59 | loss: 0.016   | val_0_val_loss: 0.01713 |  0:01:31s\n",
      "epoch 60 | loss: 0.01589 | val_0_val_loss: 0.01718 |  0:01:33s\n",
      "epoch 61 | loss: 0.01579 | val_0_val_loss: 0.01731 |  0:01:34s\n",
      "epoch 62 | loss: 0.01586 | val_0_val_loss: 0.01716 |  0:01:35s\n",
      "epoch 63 | loss: 0.01587 | val_0_val_loss: 0.01722 |  0:01:37s\n",
      "epoch 64 | loss: 0.01588 | val_0_val_loss: 0.01759 |  0:01:39s\n",
      "epoch 65 | loss: 0.01573 | val_0_val_loss: 0.01723 |  0:01:40s\n",
      "epoch 66 | loss: 0.01578 | val_0_val_loss: 0.01999 |  0:01:42s\n",
      "epoch 67 | loss: 0.01573 | val_0_val_loss: 0.01811 |  0:01:43s\n",
      "epoch 68 | loss: 0.01579 | val_0_val_loss: 0.01768 |  0:01:45s\n",
      "epoch 69 | loss: 0.01571 | val_0_val_loss: 0.01717 |  0:01:46s\n",
      "\n",
      "Early stopping occured at epoch 69 with best_epoch = 59 and best_val_0_val_loss = 0.01713\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36588 | val_0_val_loss: 0.03827 |  0:00:01s\n",
      "epoch 1  | loss: 0.02847 | val_0_val_loss: 0.02616 |  0:00:02s\n",
      "epoch 2  | loss: 0.02305 | val_0_val_loss: 0.02132 |  0:00:04s\n",
      "epoch 3  | loss: 0.02139 | val_0_val_loss: 0.02092 |  0:00:05s\n",
      "epoch 4  | loss: 0.02106 | val_0_val_loss: 0.02097 |  0:00:07s\n",
      "epoch 5  | loss: 0.02081 | val_0_val_loss: 0.0205  |  0:00:08s\n",
      "epoch 6  | loss: 0.02046 | val_0_val_loss: 0.02018 |  0:00:10s\n",
      "epoch 7  | loss: 0.02022 | val_0_val_loss: 0.01999 |  0:00:12s\n",
      "epoch 8  | loss: 0.01998 | val_0_val_loss: 0.01992 |  0:00:13s\n",
      "epoch 9  | loss: 0.01965 | val_0_val_loss: 0.01985 |  0:00:15s\n",
      "epoch 10 | loss: 0.01927 | val_0_val_loss: 0.01916 |  0:00:16s\n",
      "epoch 11 | loss: 0.01893 | val_0_val_loss: 0.01921 |  0:00:18s\n",
      "epoch 12 | loss: 0.01865 | val_0_val_loss: 0.01899 |  0:00:19s\n",
      "epoch 13 | loss: 0.01833 | val_0_val_loss: 0.01879 |  0:00:21s\n",
      "epoch 14 | loss: 0.01812 | val_0_val_loss: 0.01871 |  0:00:22s\n",
      "epoch 15 | loss: 0.01797 | val_0_val_loss: 0.0183  |  0:00:24s\n",
      "epoch 16 | loss: 0.01785 | val_0_val_loss: 0.01936 |  0:00:25s\n",
      "epoch 17 | loss: 0.01769 | val_0_val_loss: 0.01794 |  0:00:27s\n",
      "epoch 18 | loss: 0.01758 | val_0_val_loss: 0.01786 |  0:00:28s\n",
      "epoch 19 | loss: 0.0176  | val_0_val_loss: 0.01841 |  0:00:30s\n",
      "epoch 20 | loss: 0.0174  | val_0_val_loss: 0.01939 |  0:00:31s\n",
      "epoch 21 | loss: 0.01728 | val_0_val_loss: 0.01975 |  0:00:33s\n",
      "epoch 22 | loss: 0.01721 | val_0_val_loss: 0.01765 |  0:00:35s\n",
      "epoch 23 | loss: 0.01717 | val_0_val_loss: 0.01872 |  0:00:36s\n",
      "epoch 24 | loss: 0.01697 | val_0_val_loss: 0.01757 |  0:00:38s\n",
      "epoch 25 | loss: 0.01687 | val_0_val_loss: 0.02023 |  0:00:39s\n",
      "epoch 26 | loss: 0.01687 | val_0_val_loss: 0.01859 |  0:00:41s\n",
      "epoch 27 | loss: 0.01675 | val_0_val_loss: 0.02076 |  0:00:42s\n",
      "epoch 28 | loss: 0.01676 | val_0_val_loss: 0.01989 |  0:00:44s\n",
      "epoch 29 | loss: 0.01659 | val_0_val_loss: 0.01732 |  0:00:45s\n",
      "epoch 30 | loss: 0.01657 | val_0_val_loss: 0.01755 |  0:00:47s\n",
      "epoch 31 | loss: 0.01661 | val_0_val_loss: 0.01763 |  0:00:49s\n",
      "epoch 32 | loss: 0.01662 | val_0_val_loss: 0.01783 |  0:00:50s\n",
      "epoch 33 | loss: 0.0164  | val_0_val_loss: 0.01723 |  0:00:52s\n",
      "epoch 34 | loss: 0.01633 | val_0_val_loss: 0.01809 |  0:00:53s\n",
      "epoch 35 | loss: 0.01631 | val_0_val_loss: 0.02002 |  0:00:55s\n",
      "epoch 36 | loss: 0.01626 | val_0_val_loss: 0.01958 |  0:00:56s\n",
      "epoch 37 | loss: 0.01633 | val_0_val_loss: 0.01717 |  0:00:58s\n",
      "epoch 38 | loss: 0.01629 | val_0_val_loss: 0.01809 |  0:00:59s\n",
      "epoch 39 | loss: 0.01617 | val_0_val_loss: 0.01753 |  0:01:01s\n",
      "epoch 40 | loss: 0.01611 | val_0_val_loss: 0.01713 |  0:01:02s\n",
      "epoch 41 | loss: 0.01621 | val_0_val_loss: 0.01711 |  0:01:04s\n",
      "epoch 42 | loss: 0.01617 | val_0_val_loss: 0.01791 |  0:01:05s\n",
      "epoch 43 | loss: 0.01614 | val_0_val_loss: 0.01754 |  0:01:07s\n",
      "epoch 44 | loss: 0.01618 | val_0_val_loss: 0.01865 |  0:01:09s\n",
      "epoch 45 | loss: 0.01615 | val_0_val_loss: 0.01699 |  0:01:10s\n",
      "epoch 46 | loss: 0.01609 | val_0_val_loss: 0.01729 |  0:01:12s\n",
      "epoch 47 | loss: 0.01602 | val_0_val_loss: 0.01692 |  0:01:13s\n",
      "epoch 48 | loss: 0.01598 | val_0_val_loss: 0.01784 |  0:01:15s\n",
      "epoch 49 | loss: 0.0163  | val_0_val_loss: 0.01726 |  0:01:16s\n",
      "epoch 50 | loss: 0.01612 | val_0_val_loss: 0.01696 |  0:01:18s\n",
      "epoch 51 | loss: 0.01592 | val_0_val_loss: 0.01739 |  0:01:19s\n",
      "epoch 52 | loss: 0.01593 | val_0_val_loss: 0.01707 |  0:01:21s\n",
      "epoch 53 | loss: 0.01587 | val_0_val_loss: 0.0172  |  0:01:22s\n",
      "epoch 54 | loss: 0.01582 | val_0_val_loss: 0.0172  |  0:01:24s\n",
      "epoch 55 | loss: 0.01597 | val_0_val_loss: 0.01708 |  0:01:25s\n",
      "epoch 56 | loss: 0.01597 | val_0_val_loss: 0.01696 |  0:01:27s\n",
      "epoch 57 | loss: 0.01589 | val_0_val_loss: 0.01696 |  0:01:28s\n",
      "\n",
      "Early stopping occured at epoch 57 with best_epoch = 47 and best_val_0_val_loss = 0.01692\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36097 | val_0_val_loss: 0.03903 |  0:00:01s\n",
      "epoch 1  | loss: 0.02911 | val_0_val_loss: 0.02745 |  0:00:03s\n",
      "epoch 2  | loss: 0.02337 | val_0_val_loss: 0.02203 |  0:00:04s\n",
      "epoch 3  | loss: 0.02156 | val_0_val_loss: 0.02128 |  0:00:06s\n",
      "epoch 4  | loss: 0.02112 | val_0_val_loss: 0.02096 |  0:00:07s\n",
      "epoch 5  | loss: 0.02082 | val_0_val_loss: 0.02066 |  0:00:09s\n",
      "epoch 6  | loss: 0.0205  | val_0_val_loss: 0.02043 |  0:00:10s\n",
      "epoch 7  | loss: 0.02025 | val_0_val_loss: 0.02029 |  0:00:12s\n",
      "epoch 8  | loss: 0.02006 | val_0_val_loss: 0.02014 |  0:00:13s\n",
      "epoch 9  | loss: 0.01977 | val_0_val_loss: 0.02007 |  0:00:15s\n",
      "epoch 10 | loss: 0.01952 | val_0_val_loss: 0.02095 |  0:00:17s\n",
      "epoch 11 | loss: 0.0192  | val_0_val_loss: 0.01959 |  0:00:18s\n",
      "epoch 12 | loss: 0.01884 | val_0_val_loss: 0.02093 |  0:00:20s\n",
      "epoch 13 | loss: 0.01862 | val_0_val_loss: 0.01975 |  0:00:22s\n",
      "epoch 14 | loss: 0.01829 | val_0_val_loss: 0.01876 |  0:00:23s\n",
      "epoch 15 | loss: 0.01811 | val_0_val_loss: 0.01881 |  0:00:25s\n",
      "epoch 16 | loss: 0.01798 | val_0_val_loss: 0.02044 |  0:00:26s\n",
      "epoch 17 | loss: 0.01786 | val_0_val_loss: 0.01825 |  0:00:28s\n",
      "epoch 18 | loss: 0.01776 | val_0_val_loss: 0.01863 |  0:00:30s\n",
      "epoch 19 | loss: 0.0176  | val_0_val_loss: 0.0195  |  0:00:31s\n",
      "epoch 20 | loss: 0.01742 | val_0_val_loss: 0.01819 |  0:00:33s\n",
      "epoch 21 | loss: 0.01735 | val_0_val_loss: 0.02063 |  0:00:35s\n",
      "epoch 22 | loss: 0.01759 | val_0_val_loss: 0.01962 |  0:00:36s\n",
      "epoch 23 | loss: 0.01731 | val_0_val_loss: 0.02015 |  0:00:38s\n",
      "epoch 24 | loss: 0.01715 | val_0_val_loss: 0.0179  |  0:00:39s\n",
      "epoch 25 | loss: 0.01712 | val_0_val_loss: 0.01783 |  0:00:41s\n",
      "epoch 26 | loss: 0.01705 | val_0_val_loss: 0.01821 |  0:00:42s\n",
      "epoch 27 | loss: 0.01692 | val_0_val_loss: 0.01983 |  0:00:44s\n",
      "epoch 28 | loss: 0.01683 | val_0_val_loss: 0.01967 |  0:00:45s\n",
      "epoch 29 | loss: 0.01685 | val_0_val_loss: 0.01862 |  0:00:47s\n",
      "epoch 30 | loss: 0.01664 | val_0_val_loss: 0.01774 |  0:00:49s\n",
      "epoch 31 | loss: 0.01656 | val_0_val_loss: 0.01787 |  0:00:50s\n",
      "epoch 32 | loss: 0.0166  | val_0_val_loss: 0.02003 |  0:00:52s\n",
      "epoch 33 | loss: 0.01656 | val_0_val_loss: 0.01953 |  0:00:53s\n",
      "epoch 34 | loss: 0.01649 | val_0_val_loss: 0.01845 |  0:00:55s\n",
      "epoch 35 | loss: 0.01641 | val_0_val_loss: 0.01984 |  0:00:56s\n",
      "epoch 36 | loss: 0.01639 | val_0_val_loss: 0.01749 |  0:00:58s\n",
      "epoch 37 | loss: 0.01633 | val_0_val_loss: 0.01762 |  0:00:59s\n",
      "epoch 38 | loss: 0.01635 | val_0_val_loss: 0.01766 |  0:01:01s\n",
      "epoch 39 | loss: 0.01629 | val_0_val_loss: 0.01758 |  0:01:02s\n",
      "epoch 40 | loss: 0.01636 | val_0_val_loss: 0.01934 |  0:01:04s\n",
      "epoch 41 | loss: 0.01629 | val_0_val_loss: 0.02034 |  0:01:05s\n",
      "epoch 42 | loss: 0.01637 | val_0_val_loss: 0.0181  |  0:01:07s\n",
      "epoch 43 | loss: 0.01623 | val_0_val_loss: 0.01728 |  0:01:08s\n",
      "epoch 44 | loss: 0.01612 | val_0_val_loss: 0.01785 |  0:01:10s\n",
      "epoch 45 | loss: 0.01621 | val_0_val_loss: 0.0174  |  0:01:11s\n",
      "epoch 46 | loss: 0.01616 | val_0_val_loss: 0.01759 |  0:01:13s\n",
      "epoch 47 | loss: 0.01611 | val_0_val_loss: 0.01788 |  0:01:15s\n",
      "epoch 48 | loss: 0.01617 | val_0_val_loss: 0.0174  |  0:01:16s\n",
      "epoch 49 | loss: 0.01616 | val_0_val_loss: 0.01771 |  0:01:18s\n",
      "epoch 50 | loss: 0.01602 | val_0_val_loss: 0.01787 |  0:01:19s\n",
      "epoch 51 | loss: 0.01616 | val_0_val_loss: 0.01767 |  0:01:21s\n",
      "epoch 52 | loss: 0.01593 | val_0_val_loss: 0.01852 |  0:01:22s\n",
      "epoch 53 | loss: 0.01585 | val_0_val_loss: 0.01887 |  0:01:24s\n",
      "\n",
      "Early stopping occured at epoch 53 with best_epoch = 43 and best_val_0_val_loss = 0.01728\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:  6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36335 | val_0_val_loss: 0.03747 |  0:00:01s\n",
      "epoch 1  | loss: 0.02876 | val_0_val_loss: 0.02738 |  0:00:02s\n",
      "epoch 2  | loss: 0.02314 | val_0_val_loss: 0.02173 |  0:00:04s\n",
      "epoch 3  | loss: 0.02148 | val_0_val_loss: 0.02095 |  0:00:06s\n",
      "epoch 4  | loss: 0.02093 | val_0_val_loss: 0.02059 |  0:00:07s\n",
      "epoch 5  | loss: 0.02067 | val_0_val_loss: 0.02049 |  0:00:09s\n",
      "epoch 6  | loss: 0.02053 | val_0_val_loss: 0.02033 |  0:00:10s\n",
      "epoch 7  | loss: 0.02029 | val_0_val_loss: 0.02016 |  0:00:12s\n",
      "epoch 8  | loss: 0.02007 | val_0_val_loss: 0.0201  |  0:00:13s\n",
      "epoch 9  | loss: 0.01981 | val_0_val_loss: 0.01981 |  0:00:15s\n",
      "epoch 10 | loss: 0.01956 | val_0_val_loss: 0.01952 |  0:00:16s\n",
      "epoch 11 | loss: 0.01924 | val_0_val_loss: 0.01941 |  0:00:18s\n",
      "epoch 12 | loss: 0.01887 | val_0_val_loss: 0.01879 |  0:00:19s\n",
      "epoch 13 | loss: 0.01861 | val_0_val_loss: 0.01878 |  0:00:21s\n",
      "epoch 14 | loss: 0.01827 | val_0_val_loss: 0.02002 |  0:00:22s\n",
      "epoch 15 | loss: 0.01809 | val_0_val_loss: 0.01867 |  0:00:24s\n",
      "epoch 16 | loss: 0.01791 | val_0_val_loss: 0.01845 |  0:00:26s\n",
      "epoch 17 | loss: 0.01767 | val_0_val_loss: 0.01788 |  0:00:27s\n",
      "epoch 18 | loss: 0.01758 | val_0_val_loss: 0.0187  |  0:00:29s\n",
      "epoch 19 | loss: 0.01748 | val_0_val_loss: 0.01857 |  0:00:30s\n",
      "epoch 20 | loss: 0.01742 | val_0_val_loss: 0.0198  |  0:00:32s\n",
      "epoch 21 | loss: 0.01729 | val_0_val_loss: 0.01904 |  0:00:33s\n",
      "epoch 22 | loss: 0.01719 | val_0_val_loss: 0.01943 |  0:00:35s\n",
      "epoch 23 | loss: 0.0171  | val_0_val_loss: 0.01762 |  0:00:37s\n",
      "epoch 24 | loss: 0.01711 | val_0_val_loss: 0.01762 |  0:00:38s\n",
      "epoch 25 | loss: 0.01687 | val_0_val_loss: 0.0191  |  0:00:40s\n",
      "epoch 26 | loss: 0.01691 | val_0_val_loss: 0.01834 |  0:00:41s\n",
      "epoch 27 | loss: 0.01686 | val_0_val_loss: 0.01887 |  0:00:43s\n",
      "epoch 28 | loss: 0.01672 | val_0_val_loss: 0.01843 |  0:00:44s\n",
      "epoch 29 | loss: 0.01664 | val_0_val_loss: 0.01714 |  0:00:46s\n",
      "epoch 30 | loss: 0.01668 | val_0_val_loss: 0.0185  |  0:00:47s\n",
      "epoch 31 | loss: 0.01661 | val_0_val_loss: 0.01743 |  0:00:49s\n",
      "epoch 32 | loss: 0.01659 | val_0_val_loss: 0.01981 |  0:00:50s\n",
      "epoch 33 | loss: 0.01656 | val_0_val_loss: 0.01866 |  0:00:52s\n",
      "epoch 34 | loss: 0.01642 | val_0_val_loss: 0.01713 |  0:00:53s\n",
      "epoch 35 | loss: 0.01627 | val_0_val_loss: 0.01986 |  0:00:55s\n",
      "epoch 36 | loss: 0.0162  | val_0_val_loss: 0.01933 |  0:00:57s\n",
      "epoch 37 | loss: 0.01633 | val_0_val_loss: 0.01933 |  0:00:58s\n",
      "epoch 38 | loss: 0.01623 | val_0_val_loss: 0.01844 |  0:01:00s\n",
      "epoch 39 | loss: 0.01624 | val_0_val_loss: 0.01737 |  0:01:02s\n",
      "epoch 40 | loss: 0.01621 | val_0_val_loss: 0.01734 |  0:01:03s\n",
      "epoch 41 | loss: 0.01629 | val_0_val_loss: 0.01712 |  0:01:04s\n",
      "epoch 42 | loss: 0.01618 | val_0_val_loss: 0.01717 |  0:01:06s\n",
      "epoch 43 | loss: 0.01615 | val_0_val_loss: 0.01826 |  0:01:07s\n",
      "epoch 44 | loss: 0.0161  | val_0_val_loss: 0.01699 |  0:01:09s\n",
      "epoch 45 | loss: 0.01608 | val_0_val_loss: 0.01699 |  0:01:11s\n",
      "epoch 46 | loss: 0.01607 | val_0_val_loss: 0.01745 |  0:01:12s\n",
      "epoch 47 | loss: 0.01603 | val_0_val_loss: 0.01711 |  0:01:14s\n",
      "epoch 48 | loss: 0.01604 | val_0_val_loss: 0.01748 |  0:01:15s\n",
      "epoch 49 | loss: 0.01606 | val_0_val_loss: 0.017   |  0:01:17s\n",
      "epoch 50 | loss: 0.01598 | val_0_val_loss: 0.01693 |  0:01:18s\n",
      "epoch 51 | loss: 0.01588 | val_0_val_loss: 0.0167  |  0:01:20s\n",
      "epoch 52 | loss: 0.01577 | val_0_val_loss: 0.01711 |  0:01:21s\n",
      "epoch 53 | loss: 0.0158  | val_0_val_loss: 0.01707 |  0:01:23s\n",
      "epoch 54 | loss: 0.01587 | val_0_val_loss: 0.01702 |  0:01:25s\n",
      "epoch 55 | loss: 0.01593 | val_0_val_loss: 0.01705 |  0:01:27s\n",
      "epoch 56 | loss: 0.01579 | val_0_val_loss: 0.01749 |  0:01:29s\n",
      "epoch 57 | loss: 0.01576 | val_0_val_loss: 0.01728 |  0:01:30s\n",
      "epoch 58 | loss: 0.01587 | val_0_val_loss: 0.01831 |  0:01:32s\n",
      "epoch 59 | loss: 0.01579 | val_0_val_loss: 0.01693 |  0:01:34s\n",
      "epoch 60 | loss: 0.01585 | val_0_val_loss: 0.01674 |  0:01:36s\n",
      "epoch 61 | loss: 0.01572 | val_0_val_loss: 0.01672 |  0:01:38s\n",
      "\n",
      "Early stopping occured at epoch 61 with best_epoch = 51 and best_val_0_val_loss = 0.0167\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  7\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36218 | val_0_val_loss: 0.03814 |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.02845 | val_0_val_loss: 0.02553 |  0:00:03s\n",
      "epoch 2  | loss: 0.02257 | val_0_val_loss: 0.02154 |  0:00:06s\n",
      "epoch 3  | loss: 0.02144 | val_0_val_loss: 0.02116 |  0:00:08s\n",
      "epoch 4  | loss: 0.02112 | val_0_val_loss: 0.02096 |  0:00:10s\n",
      "epoch 5  | loss: 0.02084 | val_0_val_loss: 0.02064 |  0:00:11s\n",
      "epoch 6  | loss: 0.02051 | val_0_val_loss: 0.02027 |  0:00:13s\n",
      "epoch 7  | loss: 0.02014 | val_0_val_loss: 0.0212  |  0:00:15s\n",
      "epoch 8  | loss: 0.01971 | val_0_val_loss: 0.01956 |  0:00:17s\n",
      "epoch 9  | loss: 0.01932 | val_0_val_loss: 0.01909 |  0:00:19s\n",
      "epoch 10 | loss: 0.0189  | val_0_val_loss: 0.01969 |  0:00:20s\n",
      "epoch 11 | loss: 0.0186  | val_0_val_loss: 0.0191  |  0:00:22s\n",
      "epoch 12 | loss: 0.01841 | val_0_val_loss: 0.01866 |  0:00:24s\n",
      "epoch 13 | loss: 0.01817 | val_0_val_loss: 0.01848 |  0:00:26s\n",
      "epoch 14 | loss: 0.01801 | val_0_val_loss: 0.01816 |  0:00:27s\n",
      "epoch 15 | loss: 0.01785 | val_0_val_loss: 0.0184  |  0:00:29s\n",
      "epoch 16 | loss: 0.01774 | val_0_val_loss: 0.01832 |  0:00:30s\n",
      "epoch 17 | loss: 0.01766 | val_0_val_loss: 0.01804 |  0:00:32s\n",
      "epoch 18 | loss: 0.01753 | val_0_val_loss: 0.01824 |  0:00:34s\n",
      "epoch 19 | loss: 0.01741 | val_0_val_loss: 0.01919 |  0:00:35s\n",
      "epoch 20 | loss: 0.01741 | val_0_val_loss: 0.01767 |  0:00:37s\n",
      "epoch 21 | loss: 0.01724 | val_0_val_loss: 0.0178  |  0:00:39s\n",
      "epoch 22 | loss: 0.01723 | val_0_val_loss: 0.01788 |  0:00:40s\n",
      "epoch 23 | loss: 0.01723 | val_0_val_loss: 0.01956 |  0:00:42s\n",
      "epoch 24 | loss: 0.01738 | val_0_val_loss: 0.02001 |  0:00:44s\n",
      "epoch 25 | loss: 0.01708 | val_0_val_loss: 0.01779 |  0:00:46s\n",
      "epoch 26 | loss: 0.01697 | val_0_val_loss: 0.01821 |  0:00:48s\n",
      "epoch 27 | loss: 0.01699 | val_0_val_loss: 0.0173  |  0:00:49s\n",
      "epoch 28 | loss: 0.01684 | val_0_val_loss: 0.01732 |  0:00:51s\n",
      "epoch 29 | loss: 0.01676 | val_0_val_loss: 0.01723 |  0:00:53s\n",
      "epoch 30 | loss: 0.01673 | val_0_val_loss: 0.01886 |  0:00:55s\n",
      "epoch 31 | loss: 0.01667 | val_0_val_loss: 0.01728 |  0:00:57s\n",
      "epoch 32 | loss: 0.01669 | val_0_val_loss: 0.01752 |  0:00:58s\n",
      "epoch 33 | loss: 0.01689 | val_0_val_loss: 0.01731 |  0:01:00s\n",
      "epoch 34 | loss: 0.01672 | val_0_val_loss: 0.01732 |  0:01:02s\n",
      "epoch 35 | loss: 0.01646 | val_0_val_loss: 0.01719 |  0:01:03s\n",
      "epoch 36 | loss: 0.01649 | val_0_val_loss: 0.01726 |  0:01:05s\n",
      "epoch 37 | loss: 0.01637 | val_0_val_loss: 0.01764 |  0:01:07s\n",
      "epoch 38 | loss: 0.01644 | val_0_val_loss: 0.01735 |  0:01:09s\n",
      "epoch 39 | loss: 0.01635 | val_0_val_loss: 0.01736 |  0:01:11s\n",
      "epoch 40 | loss: 0.01634 | val_0_val_loss: 0.01719 |  0:01:12s\n",
      "epoch 41 | loss: 0.01632 | val_0_val_loss: 0.01796 |  0:01:14s\n",
      "epoch 42 | loss: 0.01633 | val_0_val_loss: 0.01751 |  0:01:16s\n",
      "epoch 43 | loss: 0.01633 | val_0_val_loss: 0.01774 |  0:01:18s\n",
      "epoch 44 | loss: 0.01624 | val_0_val_loss: 0.01749 |  0:01:19s\n",
      "epoch 45 | loss: 0.01629 | val_0_val_loss: 0.01724 |  0:01:21s\n",
      "epoch 46 | loss: 0.01625 | val_0_val_loss: 0.02079 |  0:01:23s\n",
      "epoch 47 | loss: 0.01618 | val_0_val_loss: 0.01721 |  0:01:24s\n",
      "epoch 48 | loss: 0.01623 | val_0_val_loss: 0.01696 |  0:01:26s\n",
      "epoch 49 | loss: 0.01618 | val_0_val_loss: 0.01711 |  0:01:27s\n",
      "epoch 50 | loss: 0.0161  | val_0_val_loss: 0.01693 |  0:01:29s\n",
      "epoch 51 | loss: 0.01594 | val_0_val_loss: 0.017   |  0:01:31s\n",
      "epoch 52 | loss: 0.01606 | val_0_val_loss: 0.01684 |  0:01:33s\n",
      "epoch 53 | loss: 0.016   | val_0_val_loss: 0.01691 |  0:01:34s\n",
      "epoch 54 | loss: 0.01596 | val_0_val_loss: 0.01716 |  0:01:36s\n",
      "epoch 55 | loss: 0.01593 | val_0_val_loss: 0.01711 |  0:01:38s\n",
      "epoch 56 | loss: 0.01601 | val_0_val_loss: 0.01868 |  0:01:39s\n",
      "epoch 57 | loss: 0.01603 | val_0_val_loss: 0.01725 |  0:01:41s\n",
      "epoch 58 | loss: 0.0159  | val_0_val_loss: 0.01681 |  0:01:43s\n",
      "epoch 59 | loss: 0.01577 | val_0_val_loss: 0.0172  |  0:01:45s\n",
      "epoch 60 | loss: 0.01588 | val_0_val_loss: 0.01674 |  0:01:46s\n",
      "epoch 61 | loss: 0.01596 | val_0_val_loss: 0.01687 |  0:01:48s\n",
      "epoch 62 | loss: 0.01585 | val_0_val_loss: 0.01675 |  0:01:50s\n",
      "epoch 63 | loss: 0.01572 | val_0_val_loss: 0.01682 |  0:01:52s\n",
      "epoch 64 | loss: 0.01581 | val_0_val_loss: 0.017   |  0:01:53s\n",
      "epoch 65 | loss: 0.0159  | val_0_val_loss: 0.01688 |  0:01:55s\n",
      "epoch 66 | loss: 0.01589 | val_0_val_loss: 0.01663 |  0:01:57s\n",
      "epoch 67 | loss: 0.01581 | val_0_val_loss: 0.01781 |  0:01:59s\n",
      "epoch 68 | loss: 0.01585 | val_0_val_loss: 0.01701 |  0:02:00s\n",
      "epoch 69 | loss: 0.01588 | val_0_val_loss: 0.01679 |  0:02:02s\n",
      "epoch 70 | loss: 0.01581 | val_0_val_loss: 0.01694 |  0:02:04s\n",
      "epoch 71 | loss: 0.01573 | val_0_val_loss: 0.0167  |  0:02:06s\n",
      "epoch 72 | loss: 0.01576 | val_0_val_loss: 0.01685 |  0:02:07s\n",
      "epoch 73 | loss: 0.01568 | val_0_val_loss: 0.01684 |  0:02:09s\n",
      "epoch 74 | loss: 0.01573 | val_0_val_loss: 0.017   |  0:02:11s\n",
      "epoch 75 | loss: 0.01587 | val_0_val_loss: 0.01681 |  0:02:13s\n",
      "epoch 76 | loss: 0.01581 | val_0_val_loss: 0.01681 |  0:02:14s\n",
      "\n",
      "Early stopping occured at epoch 76 with best_epoch = 66 and best_val_0_val_loss = 0.01663\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  8\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36514 | val_0_val_loss: 0.03791 |  0:00:01s\n",
      "epoch 1  | loss: 0.02855 | val_0_val_loss: 0.02732 |  0:00:03s\n",
      "epoch 2  | loss: 0.023   | val_0_val_loss: 0.02182 |  0:00:05s\n",
      "epoch 3  | loss: 0.02123 | val_0_val_loss: 0.0211  |  0:00:06s\n",
      "epoch 4  | loss: 0.02083 | val_0_val_loss: 0.02079 |  0:00:08s\n",
      "epoch 5  | loss: 0.02049 | val_0_val_loss: 0.02055 |  0:00:10s\n",
      "epoch 6  | loss: 0.02023 | val_0_val_loss: 0.02033 |  0:00:12s\n",
      "epoch 7  | loss: 0.01997 | val_0_val_loss: 0.02006 |  0:00:13s\n",
      "epoch 8  | loss: 0.01963 | val_0_val_loss: 0.01969 |  0:00:15s\n",
      "epoch 9  | loss: 0.01926 | val_0_val_loss: 0.01964 |  0:00:17s\n",
      "epoch 10 | loss: 0.01889 | val_0_val_loss: 0.0188  |  0:00:19s\n",
      "epoch 11 | loss: 0.01857 | val_0_val_loss: 0.01886 |  0:00:21s\n",
      "epoch 12 | loss: 0.0184  | val_0_val_loss: 0.01941 |  0:00:23s\n",
      "epoch 13 | loss: 0.01822 | val_0_val_loss: 0.01994 |  0:00:24s\n",
      "epoch 14 | loss: 0.01801 | val_0_val_loss: 0.01805 |  0:00:26s\n",
      "epoch 15 | loss: 0.01791 | val_0_val_loss: 0.0218  |  0:00:28s\n",
      "epoch 16 | loss: 0.01775 | val_0_val_loss: 0.01949 |  0:00:30s\n",
      "epoch 17 | loss: 0.01766 | val_0_val_loss: 0.0212  |  0:00:31s\n",
      "epoch 18 | loss: 0.01758 | val_0_val_loss: 0.01961 |  0:00:33s\n",
      "epoch 19 | loss: 0.01749 | val_0_val_loss: 0.01788 |  0:00:35s\n",
      "epoch 20 | loss: 0.01737 | val_0_val_loss: 0.02014 |  0:00:36s\n",
      "epoch 21 | loss: 0.01735 | val_0_val_loss: 0.0199  |  0:00:38s\n",
      "epoch 22 | loss: 0.01732 | val_0_val_loss: 0.01799 |  0:00:40s\n",
      "epoch 23 | loss: 0.01719 | val_0_val_loss: 0.01854 |  0:00:42s\n",
      "epoch 24 | loss: 0.01706 | val_0_val_loss: 0.01977 |  0:00:44s\n",
      "epoch 25 | loss: 0.017   | val_0_val_loss: 0.02141 |  0:00:46s\n",
      "epoch 26 | loss: 0.01702 | val_0_val_loss: 0.01942 |  0:00:47s\n",
      "epoch 27 | loss: 0.01689 | val_0_val_loss: 0.02021 |  0:00:49s\n",
      "epoch 28 | loss: 0.01689 | val_0_val_loss: 0.01783 |  0:00:51s\n",
      "epoch 29 | loss: 0.01699 | val_0_val_loss: 0.01836 |  0:00:53s\n",
      "epoch 30 | loss: 0.01682 | val_0_val_loss: 0.0218  |  0:00:54s\n",
      "epoch 31 | loss: 0.01664 | val_0_val_loss: 0.01921 |  0:00:56s\n",
      "epoch 32 | loss: 0.01674 | val_0_val_loss: 0.01966 |  0:00:58s\n",
      "epoch 33 | loss: 0.01669 | val_0_val_loss: 0.01791 |  0:01:00s\n",
      "epoch 34 | loss: 0.01671 | val_0_val_loss: 0.01801 |  0:01:01s\n",
      "epoch 35 | loss: 0.01657 | val_0_val_loss: 0.01734 |  0:01:03s\n",
      "epoch 36 | loss: 0.01648 | val_0_val_loss: 0.01956 |  0:01:05s\n",
      "epoch 37 | loss: 0.01646 | val_0_val_loss: 0.01898 |  0:01:08s\n",
      "epoch 38 | loss: 0.01643 | val_0_val_loss: 0.01798 |  0:01:10s\n",
      "epoch 39 | loss: 0.01643 | val_0_val_loss: 0.01921 |  0:01:12s\n",
      "epoch 40 | loss: 0.01642 | val_0_val_loss: 0.01779 |  0:01:14s\n",
      "epoch 41 | loss: 0.01636 | val_0_val_loss: 0.01826 |  0:01:16s\n",
      "epoch 42 | loss: 0.01634 | val_0_val_loss: 0.01914 |  0:01:17s\n",
      "epoch 43 | loss: 0.01642 | val_0_val_loss: 0.01746 |  0:01:19s\n",
      "epoch 44 | loss: 0.01633 | val_0_val_loss: 0.01778 |  0:01:21s\n",
      "epoch 45 | loss: 0.01627 | val_0_val_loss: 0.01871 |  0:01:23s\n",
      "\n",
      "Early stopping occured at epoch 45 with best_epoch = 35 and best_val_0_val_loss = 0.01734\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  9\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36746 | val_0_val_loss: 0.0432  |  0:00:01s\n",
      "epoch 1  | loss: 0.02901 | val_0_val_loss: 0.02797 |  0:00:03s\n",
      "epoch 2  | loss: 0.02342 | val_0_val_loss: 0.02131 |  0:00:05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.02138 | val_0_val_loss: 0.0208  |  0:00:07s\n",
      "epoch 4  | loss: 0.02092 | val_0_val_loss: 0.02051 |  0:00:08s\n",
      "epoch 5  | loss: 0.02065 | val_0_val_loss: 0.0203  |  0:00:10s\n",
      "epoch 6  | loss: 0.02027 | val_0_val_loss: 0.02012 |  0:00:12s\n",
      "epoch 7  | loss: 0.02004 | val_0_val_loss: 0.01979 |  0:00:13s\n",
      "epoch 8  | loss: 0.01967 | val_0_val_loss: 0.0196  |  0:00:15s\n",
      "epoch 9  | loss: 0.01936 | val_0_val_loss: 0.01923 |  0:00:17s\n",
      "epoch 10 | loss: 0.01894 | val_0_val_loss: 0.01956 |  0:00:19s\n",
      "epoch 11 | loss: 0.01868 | val_0_val_loss: 0.01999 |  0:00:20s\n",
      "epoch 12 | loss: 0.01834 | val_0_val_loss: 0.01867 |  0:00:22s\n",
      "epoch 13 | loss: 0.01809 | val_0_val_loss: 0.01871 |  0:00:24s\n",
      "epoch 14 | loss: 0.01787 | val_0_val_loss: 0.01941 |  0:00:26s\n",
      "epoch 15 | loss: 0.01775 | val_0_val_loss: 0.02055 |  0:00:28s\n",
      "epoch 16 | loss: 0.01753 | val_0_val_loss: 0.01867 |  0:00:29s\n",
      "epoch 17 | loss: 0.01741 | val_0_val_loss: 0.0179  |  0:00:31s\n",
      "epoch 18 | loss: 0.01728 | val_0_val_loss: 0.01798 |  0:00:33s\n",
      "epoch 19 | loss: 0.01723 | val_0_val_loss: 0.01772 |  0:00:35s\n",
      "epoch 20 | loss: 0.01703 | val_0_val_loss: 0.01766 |  0:00:36s\n",
      "epoch 21 | loss: 0.01686 | val_0_val_loss: 0.01754 |  0:00:38s\n",
      "epoch 22 | loss: 0.01679 | val_0_val_loss: 0.01962 |  0:00:40s\n",
      "epoch 23 | loss: 0.0168  | val_0_val_loss: 0.01757 |  0:00:42s\n",
      "epoch 24 | loss: 0.01672 | val_0_val_loss: 0.01782 |  0:00:43s\n",
      "epoch 25 | loss: 0.01672 | val_0_val_loss: 0.01887 |  0:00:45s\n",
      "epoch 26 | loss: 0.01661 | val_0_val_loss: 0.01775 |  0:00:47s\n",
      "epoch 27 | loss: 0.01661 | val_0_val_loss: 0.01727 |  0:00:49s\n",
      "epoch 28 | loss: 0.01652 | val_0_val_loss: 0.01761 |  0:00:50s\n",
      "epoch 29 | loss: 0.01651 | val_0_val_loss: 0.02015 |  0:00:52s\n",
      "epoch 30 | loss: 0.01655 | val_0_val_loss: 0.01772 |  0:00:54s\n",
      "epoch 31 | loss: 0.01642 | val_0_val_loss: 0.01721 |  0:00:56s\n",
      "epoch 32 | loss: 0.01635 | val_0_val_loss: 0.01757 |  0:00:58s\n",
      "epoch 33 | loss: 0.01625 | val_0_val_loss: 0.01925 |  0:00:59s\n",
      "epoch 34 | loss: 0.01621 | val_0_val_loss: 0.01749 |  0:01:01s\n",
      "epoch 35 | loss: 0.01628 | val_0_val_loss: 0.01721 |  0:01:03s\n",
      "epoch 36 | loss: 0.0162  | val_0_val_loss: 0.01784 |  0:01:05s\n",
      "epoch 37 | loss: 0.01622 | val_0_val_loss: 0.01727 |  0:01:07s\n",
      "epoch 38 | loss: 0.0161  | val_0_val_loss: 0.01718 |  0:01:08s\n",
      "epoch 39 | loss: 0.01616 | val_0_val_loss: 0.01724 |  0:01:10s\n",
      "epoch 40 | loss: 0.0162  | val_0_val_loss: 0.01796 |  0:01:12s\n",
      "epoch 41 | loss: 0.01608 | val_0_val_loss: 0.01719 |  0:01:14s\n",
      "epoch 42 | loss: 0.01608 | val_0_val_loss: 0.01846 |  0:01:15s\n",
      "epoch 43 | loss: 0.01602 | val_0_val_loss: 0.01763 |  0:01:17s\n",
      "epoch 44 | loss: 0.01613 | val_0_val_loss: 0.01753 |  0:01:19s\n",
      "epoch 45 | loss: 0.01604 | val_0_val_loss: 0.01743 |  0:01:21s\n",
      "epoch 46 | loss: 0.01607 | val_0_val_loss: 0.01819 |  0:01:22s\n",
      "epoch 47 | loss: 0.01598 | val_0_val_loss: 0.01791 |  0:01:24s\n",
      "epoch 48 | loss: 0.01597 | val_0_val_loss: 0.01695 |  0:01:25s\n",
      "epoch 49 | loss: 0.01616 | val_0_val_loss: 0.01833 |  0:01:27s\n",
      "epoch 50 | loss: 0.01601 | val_0_val_loss: 0.01726 |  0:01:29s\n",
      "epoch 51 | loss: 0.01596 | val_0_val_loss: 0.01708 |  0:01:30s\n",
      "epoch 52 | loss: 0.01582 | val_0_val_loss: 0.01913 |  0:01:32s\n",
      "epoch 53 | loss: 0.01577 | val_0_val_loss: 0.01815 |  0:01:34s\n",
      "epoch 54 | loss: 0.01573 | val_0_val_loss: 0.01715 |  0:01:36s\n",
      "epoch 55 | loss: 0.01586 | val_0_val_loss: 0.01732 |  0:01:38s\n",
      "epoch 56 | loss: 0.01592 | val_0_val_loss: 0.01721 |  0:01:39s\n",
      "epoch 57 | loss: 0.01579 | val_0_val_loss: 0.01684 |  0:01:41s\n",
      "epoch 58 | loss: 0.01576 | val_0_val_loss: 0.01833 |  0:01:43s\n",
      "epoch 59 | loss: 0.01567 | val_0_val_loss: 0.01714 |  0:01:45s\n",
      "epoch 60 | loss: 0.0157  | val_0_val_loss: 0.01842 |  0:01:46s\n",
      "epoch 61 | loss: 0.0157  | val_0_val_loss: 0.01712 |  0:01:48s\n",
      "epoch 62 | loss: 0.01573 | val_0_val_loss: 0.017   |  0:01:50s\n",
      "epoch 63 | loss: 0.0157  | val_0_val_loss: 0.01725 |  0:01:52s\n",
      "epoch 64 | loss: 0.01577 | val_0_val_loss: 0.01713 |  0:01:53s\n",
      "epoch 65 | loss: 0.01579 | val_0_val_loss: 0.0171  |  0:01:55s\n",
      "epoch 66 | loss: 0.01572 | val_0_val_loss: 0.0169  |  0:01:57s\n",
      "epoch 67 | loss: 0.01563 | val_0_val_loss: 0.01684 |  0:01:59s\n",
      "epoch 68 | loss: 0.01569 | val_0_val_loss: 0.017   |  0:02:01s\n",
      "epoch 69 | loss: 0.01569 | val_0_val_loss: 0.01694 |  0:02:02s\n",
      "epoch 70 | loss: 0.01581 | val_0_val_loss: 0.01714 |  0:02:04s\n",
      "epoch 71 | loss: 0.01591 | val_0_val_loss: 0.01691 |  0:02:06s\n",
      "epoch 72 | loss: 0.01564 | val_0_val_loss: 0.0169  |  0:02:08s\n",
      "epoch 73 | loss: 0.01567 | val_0_val_loss: 0.01819 |  0:02:10s\n",
      "epoch 74 | loss: 0.0158  | val_0_val_loss: 0.01717 |  0:02:12s\n",
      "epoch 75 | loss: 0.01572 | val_0_val_loss: 0.01698 |  0:02:13s\n",
      "epoch 76 | loss: 0.01573 | val_0_val_loss: 0.01708 |  0:02:15s\n",
      "epoch 77 | loss: 0.01569 | val_0_val_loss: 0.0178  |  0:02:17s\n",
      "\n",
      "Early stopping occured at epoch 77 with best_epoch = 67 and best_val_0_val_loss = 0.01684\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLD:  10\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36713 | val_0_val_loss: 0.03985 |  0:00:01s\n",
      "epoch 1  | loss: 0.02802 | val_0_val_loss: 0.02554 |  0:00:03s\n",
      "epoch 2  | loss: 0.02267 | val_0_val_loss: 0.02143 |  0:00:04s\n",
      "epoch 3  | loss: 0.02113 | val_0_val_loss: 0.02082 |  0:00:06s\n",
      "epoch 4  | loss: 0.02077 | val_0_val_loss: 0.02062 |  0:00:07s\n",
      "epoch 5  | loss: 0.02053 | val_0_val_loss: 0.02041 |  0:00:09s\n",
      "epoch 6  | loss: 0.0203  | val_0_val_loss: 0.0203  |  0:00:11s\n",
      "epoch 7  | loss: 0.02008 | val_0_val_loss: 0.02014 |  0:00:12s\n",
      "epoch 8  | loss: 0.01992 | val_0_val_loss: 0.01992 |  0:00:14s\n",
      "epoch 9  | loss: 0.01976 | val_0_val_loss: 0.01974 |  0:00:15s\n",
      "epoch 10 | loss: 0.0196  | val_0_val_loss: 0.01977 |  0:00:17s\n",
      "epoch 11 | loss: 0.0193  | val_0_val_loss: 0.01952 |  0:00:18s\n",
      "epoch 12 | loss: 0.01905 | val_0_val_loss: 0.01923 |  0:00:20s\n",
      "epoch 13 | loss: 0.01879 | val_0_val_loss: 0.0198  |  0:00:22s\n",
      "epoch 14 | loss: 0.01855 | val_0_val_loss: 0.01956 |  0:00:24s\n",
      "epoch 15 | loss: 0.01832 | val_0_val_loss: 0.01898 |  0:00:26s\n",
      "epoch 16 | loss: 0.01811 | val_0_val_loss: 0.0184  |  0:00:27s\n",
      "epoch 17 | loss: 0.01784 | val_0_val_loss: 0.0183  |  0:00:29s\n",
      "epoch 18 | loss: 0.01766 | val_0_val_loss: 0.01823 |  0:00:31s\n",
      "epoch 19 | loss: 0.01751 | val_0_val_loss: 0.01813 |  0:00:33s\n",
      "epoch 20 | loss: 0.01738 | val_0_val_loss: 0.01956 |  0:00:35s\n",
      "epoch 21 | loss: 0.01728 | val_0_val_loss: 0.01814 |  0:00:36s\n",
      "epoch 22 | loss: 0.01714 | val_0_val_loss: 0.0181  |  0:00:38s\n",
      "epoch 23 | loss: 0.01706 | val_0_val_loss: 0.01884 |  0:00:40s\n",
      "epoch 24 | loss: 0.01687 | val_0_val_loss: 0.01955 |  0:00:42s\n",
      "epoch 25 | loss: 0.0168  | val_0_val_loss: 0.01743 |  0:00:43s\n",
      "epoch 26 | loss: 0.01677 | val_0_val_loss: 0.02887 |  0:00:45s\n",
      "epoch 27 | loss: 0.01673 | val_0_val_loss: 0.01737 |  0:00:47s\n",
      "epoch 28 | loss: 0.01663 | val_0_val_loss: 0.0175  |  0:00:49s\n",
      "epoch 29 | loss: 0.01653 | val_0_val_loss: 0.01867 |  0:00:51s\n",
      "epoch 30 | loss: 0.01655 | val_0_val_loss: 0.01766 |  0:00:52s\n",
      "epoch 31 | loss: 0.01653 | val_0_val_loss: 0.01857 |  0:00:54s\n",
      "epoch 32 | loss: 0.01651 | val_0_val_loss: 0.01749 |  0:00:56s\n",
      "epoch 33 | loss: 0.01641 | val_0_val_loss: 0.01712 |  0:00:58s\n",
      "epoch 34 | loss: 0.01642 | val_0_val_loss: 0.01835 |  0:00:59s\n",
      "epoch 35 | loss: 0.01633 | val_0_val_loss: 0.01705 |  0:01:01s\n",
      "epoch 36 | loss: 0.01629 | val_0_val_loss: 0.01739 |  0:01:03s\n",
      "epoch 37 | loss: 0.01631 | val_0_val_loss: 0.01724 |  0:01:05s\n",
      "epoch 38 | loss: 0.01622 | val_0_val_loss: 0.01756 |  0:01:07s\n",
      "epoch 39 | loss: 0.01616 | val_0_val_loss: 0.01701 |  0:01:08s\n",
      "epoch 40 | loss: 0.01613 | val_0_val_loss: 0.01711 |  0:01:10s\n",
      "epoch 41 | loss: 0.0162  | val_0_val_loss: 0.01709 |  0:01:12s\n",
      "epoch 42 | loss: 0.01627 | val_0_val_loss: 0.01719 |  0:01:14s\n",
      "epoch 43 | loss: 0.01621 | val_0_val_loss: 0.01702 |  0:01:16s\n",
      "epoch 44 | loss: 0.01606 | val_0_val_loss: 0.01737 |  0:01:17s\n",
      "epoch 45 | loss: 0.01625 | val_0_val_loss: 0.01781 |  0:01:19s\n",
      "epoch 46 | loss: 0.01618 | val_0_val_loss: 0.01698 |  0:01:21s\n",
      "epoch 47 | loss: 0.0161  | val_0_val_loss: 0.01711 |  0:01:23s\n",
      "epoch 48 | loss: 0.01609 | val_0_val_loss: 0.01719 |  0:01:24s\n",
      "epoch 49 | loss: 0.01612 | val_0_val_loss: 0.01704 |  0:01:26s\n",
      "epoch 50 | loss: 0.01602 | val_0_val_loss: 0.01716 |  0:01:28s\n",
      "epoch 51 | loss: 0.01596 | val_0_val_loss: 0.01686 |  0:01:30s\n",
      "epoch 52 | loss: 0.0159  | val_0_val_loss: 0.01711 |  0:01:32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 | loss: 0.01602 | val_0_val_loss: 0.01753 |  0:01:33s\n",
      "epoch 54 | loss: 0.01617 | val_0_val_loss: 0.01701 |  0:01:35s\n",
      "epoch 55 | loss: 0.01595 | val_0_val_loss: 0.01697 |  0:01:37s\n",
      "epoch 56 | loss: 0.01586 | val_0_val_loss: 0.01733 |  0:01:39s\n",
      "epoch 57 | loss: 0.01588 | val_0_val_loss: 0.01696 |  0:01:41s\n",
      "epoch 58 | loss: 0.01584 | val_0_val_loss: 0.01701 |  0:01:42s\n",
      "epoch 59 | loss: 0.01583 | val_0_val_loss: 0.01675 |  0:01:44s\n",
      "epoch 60 | loss: 0.01588 | val_0_val_loss: 0.01714 |  0:01:46s\n",
      "epoch 61 | loss: 0.0159  | val_0_val_loss: 0.01677 |  0:01:48s\n",
      "epoch 62 | loss: 0.01589 | val_0_val_loss: 0.01752 |  0:01:49s\n",
      "epoch 63 | loss: 0.01585 | val_0_val_loss: 0.01754 |  0:01:51s\n",
      "epoch 64 | loss: 0.0159  | val_0_val_loss: 0.01726 |  0:01:53s\n",
      "epoch 65 | loss: 0.016   | val_0_val_loss: 0.01753 |  0:01:55s\n",
      "epoch 66 | loss: 0.0158  | val_0_val_loss: 0.01684 |  0:01:57s\n",
      "epoch 67 | loss: 0.01582 | val_0_val_loss: 0.01712 |  0:01:58s\n",
      "epoch 68 | loss: 0.01586 | val_0_val_loss: 0.01689 |  0:02:00s\n",
      "epoch 69 | loss: 0.01573 | val_0_val_loss: 0.01707 |  0:02:02s\n",
      "\n",
      "Early stopping occured at epoch 69 with best_epoch = 59 and best_val_0_val_loss = 0.01675\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "mskf = MultilabelStratifiedKFold(n_splits = n_folds, random_state = 0, shuffle = True)\n",
    "models = {}\n",
    "for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_d, lables_train)):\n",
    "    print(\"FOLD: \", fold_nb + 1)\n",
    "    \n",
    "    train_split, lables_train_split = train_d[train_idx, ::], lables_train[train_idx, ::]\n",
    "    val_split, lables_val_split = train_d[val_idx, ::], lables_train[val_idx, ::]\n",
    "    ### Model ###\n",
    "    name = \"model\"+str(fold_nb)\n",
    "    models[name] = TabNetRegressor(n_d=decision_layer_size, n_a=mask_attention_layer_size, n_steps=1, lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=tabnet_learning_rate, weight_decay=tabnet_weight_decay), mask_type='entmax', \n",
    "                                    scheduler_params=dict(milestones=[50, 100, 150], gamma=0.9), \n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "    models[name].fit(X_train=train_split, y_train=lables_train_split,\n",
    "              eval_set=[(val_split, lables_val_split)],\n",
    "              loss_fn = torch.nn.BCEWithLogitsLoss(),\n",
    "              eval_metric = [LogitsLogLoss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blender(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "            super().__init__()\n",
    "            self.scaler = torch.nn.parameter.Parameter(torch.rand(size = (NB_SPLITS, lable_size)), requires_grad=True)\n",
    "            self.sig = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, x):\n",
    "    \n",
    "        placeholder = torch.zeros(x.shape[0], lable_size).to(device)\n",
    "        \n",
    "        for i in range(NB_SPLITS):\n",
    "            placeholder += 0.2*self.scaler[i]*torch.tensor(models[\"model\"+str(i)].predict(x.cpu().detach().numpy()), requires_grad=False).to(device)\n",
    "            \n",
    "        pred = placeholder\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = Blender().to(device)\n",
    "optimizer_blender = optim.Adam(blender.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=blender_batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    validationset, batch_size=blender_batch_size, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/20, loss = 0.0181\n",
      "epoch : 2/20, loss = 0.0178\n",
      "epoch : 3/20, loss = 0.0175\n",
      "epoch : 4/20, loss = 0.0178\n",
      "epoch : 5/20, loss = 0.0185\n",
      "epoch : 6/20, loss = 0.0173\n",
      "epoch : 7/20, loss = 0.0179\n",
      "epoch : 8/20, loss = 0.0171\n",
      "epoch : 9/20, loss = 0.0173\n",
      "epoch : 10/20, loss = 0.0179\n",
      "epoch : 11/20, loss = 0.0172\n",
      "epoch : 12/20, loss = 0.0179\n",
      "epoch : 13/20, loss = 0.0167\n",
      "epoch : 14/20, loss = 0.0168\n",
      "epoch : 15/20, loss = 0.0177\n",
      "epoch : 16/20, loss = 0.0156\n",
      "epoch : 17/20, loss = 0.0127\n",
      "epoch : 18/20, loss = 0.0170\n",
      "epoch : 19/20, loss = 0.0165\n",
      "epoch : 20/20, loss = 0.0181\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "epoch_list = []\n",
    "val_list = []\n",
    "for epoch in range(epochs):    \n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer_blender.zero_grad()\n",
    "        \n",
    "        outputs = blender.forward(x.float())\n",
    "        # compute training reconstruction loss\n",
    "        train_loss = nn.BCEWithLogitsLoss()(outputs, y)\n",
    "\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer_blender.step()\n",
    " \n",
    "        train_loss = train_loss.cpu().detach().numpy()\n",
    "        val_list.append(train_loss)\n",
    "        torch.save(blender.state_dict(), \"./saves/blend\"+str(epoch))\n",
    "        epoch_list.append(epoch)\n",
    "        print(\"epoch : {}/{}, loss = {:.4f}\".format(epoch + 1, epochs, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blender(\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch = np.argmin(val_list)\n",
    "blender.load_state_dict(torch.load(\"./saves/blend\"+str(best_epoch)))\n",
    "blender.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "pred_encode = np.empty(shape = (test.shape[0], lable_size))\n",
    "for x in pred_loader:\n",
    "    x = x.to(device)\n",
    "    outputs = blender.forward(x)\n",
    "    pred_encode[((i-1)*(outputs.shape[0])):(i*(outputs.shape[0])), ::] = 1 / (1 + np.exp(-outputs.cpu().detach().numpy()))\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:54.751705Z",
     "iopub.status.busy": "2020-10-03T15:52:54.751027Z",
     "iopub.status.idle": "2020-10-03T15:52:54.790527Z",
     "shell.execute_reply": "2020-10-03T15:52:54.789901Z"
    },
    "papermill": {
     "duration": 0.071463,
     "end_time": "2020-10-03T15:52:54.790658",
     "exception": false,
     "start_time": "2020-10-03T15:52:54.719195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.010072</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.026643</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.001105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.002052                0.010072   \n",
       "1  id_001897cda                     0.001512                0.006640   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.003310                        0.005938   \n",
       "1        0.003262                        0.008240   \n",
       "2        0.000000                        0.000000   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.015749                        0.018587   \n",
       "1                           0.026643                        0.016565   \n",
       "2                           0.000000                        0.000000   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.007780                       0.002433   \n",
       "1                    0.005519                       0.002926   \n",
       "2                    0.000000                       0.000000   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000053  ...                               0.000528   \n",
       "1                    0.000028  ...                               0.000518   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.003955         0.006279           0.001776   \n",
       "1      0.003208         0.004323           0.001339   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.007222                               0.000584   \n",
       "1                   0.009365                               0.000454   \n",
       "2                   0.000000                               0.000000   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.000751   0.001802                    0.002585       0.001044  \n",
       "1         0.002070   0.002292                    0.001957       0.001105  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "\n",
       "[3 rows x 207 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a copy of all our training sig_id s for reference\n",
    "test_sig_ids = test_features['sig_id'].copy()\n",
    "\n",
    "# select all indices when 'cp_type' is 'ctl_vehicle'\n",
    "test_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')\n",
    "\n",
    "# change all cp_type == ctl_vehicle predictions to zero\n",
    "pred_encode[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n",
    "test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\n",
    "test_preds_df = pd.DataFrame(pred_encode, columns=train_targets_scored.columns[1:])\n",
    "test_submission = pd.concat([test_submission, test_preds_df], axis=1)\n",
    "test_submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-03T15:52:54.849482Z",
     "iopub.status.busy": "2020-10-03T15:52:54.848574Z",
     "iopub.status.idle": "2020-10-03T15:52:57.019352Z",
     "shell.execute_reply": "2020-10-03T15:52:57.018223Z"
    },
    "papermill": {
     "duration": 2.202021,
     "end_time": "2020-10-03T15:52:57.019481",
     "exception": false,
     "start_time": "2020-10-03T15:52:54.817460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 54.879322,
   "end_time": "2020-10-03T15:52:58.256922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-03T15:52:03.377600",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
